---
title: "mod5_mcspadden_diana"
author: "Diana McSpadden"
date: "3/14/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(tibble)
```

# Question 8
In this exercise, we will generate simulated data, and will then use this data to perform best subset selection.

## Q8 (a) 
Use the rnorm() function to generate a predictor X of length n = 100, as well as a noise vector E of length n = 100. 

```{r q8a-1}
set.seed (1976)

X = rnorm(100)

E = rnorm (100)
```


## Q8 (b) 
Generate a response vector Y of length n = 100 according to the model:

Y = β0 + β1X + β2X2 + β3X3 + E,

where β0, β1, β2, and β3 are constants of your choice.

```{r q8b-1}
B0 = 10
B1 = -13
B2 = 3.27
B3 = -9.11

Y = B0 + (B1 * X) + (B2 * X^2) + (B3 * X^3) + E

```


## Q8 (c) 
Use the regsubsets() function to perform best subset selection in order to choose the best model containing the predictors X,X2, . . .,X10. 

```{r q8c-1}
df.q8c = data.frame(x1 = X, x2 = X^2, x3 = X^3, x4 = X^4, x5 = X^5, x6 = X^6, x7 = X^7, x8 = X^8, x9 = X^9, x10 = X^10, y = Y)

head(df.q8c)
```

``` {r q8c-2}
library(leaps)
regfit.full <- leaps::regsubsets(y~., data = df.q8c, nvmax=10)
#summary(regfit.full)
regfit.full %>% broom::tidy()

reg.summary = regfit.full %>% broom::tidy() 

#reg.summary %>% names()
#reg.summary %>% dplyr::select(adj.r.squared)
#reg.summary %>% dplyr::select(BIC)
#reg.summary %>% dplyr::select(mallows_cp)
```
What is the best model obtained according to Cp, BIC, and adjusted R2? 

**Answer For Q8c, best model using regsubsets:** The best models are:
<ul>
<li>Cp: $Y = Intercept + X + X^2 + X^3 + X^4$</li>
<li>BIC: $Y = Intercept + X^3$</li>
<li>Adj R^2: $Y = Intercept + X + X^2 + X^3 + X^8 + X^{10}$</li>
</ul>

Show some plots to provide evidence for your answer, and report the coefficients of the best model obtained. Note you will need to use the data.frame() function to create a single data set containing both X and Y.

``` {r q8c-3}
adjR2s_plot = function(model.summary, xlab) {
  model.summary %>%
  ggplot(aes(x=seq_along(adj.r.squared), y=adj.r.squared)) + geom_line() +
  geom_point(model.summary, color='red', size=2, mapping=aes(x=which.max(adj.r.squared), y=max(adj.r.squared)))  + xlab(xlab) 
}

mallows_cps_plot = function(model.summary, xlab) {
  model.summary %>%
  ggplot(aes(x=seq_along(mallows_cp), y=mallows_cp)) + geom_line() +
  geom_point(model.summary, color='red', size=2, mapping=aes(x=which.min(mallows_cp), y=min(mallows_cp))) +  xlab(xlab)
}

BICs_plot = function(model.summary, xlab) {
  model.summary %>%
  ggplot(aes(x=seq_along(BIC), y=BIC)) + geom_line() +
  geom_point(model.summary, color='red', size=2, mapping=aes(x=which.min(BIC), y=min(BIC))) +  xlab(xlab) 
}
```

```{r q8c-4}
library(gridExtra)

grid.arrange(adjR2s_plot(reg.summary, "RegSubSets: Number of Variables"), mallows_cps_plot(reg.summary, "RegSubSets: Number of Variables"), BICs_plot(reg.summary, "RegSubSets: Number of Variables"), nrow=2)
```

## Q8 (d) 
Repeat (c), using forward step-wise selection and also using backwards step-wise selection. How does your answer compare to the results in (c)?

#### Forward
```{r q8d-1}
regfit.fwd <- regsubsets(y~., data=df.q8c, nvmax=10, method="forward")
#summary(regfit.fwd)
regfit.fwd.summary = regfit.fwd %>% broom::tidy()

regfit.fwd.summary

grid.arrange(adjR2s_plot(regfit.fwd.summary, "Stepwise Fwd: Number of Variables"), mallows_cps_plot(regfit.fwd.summary, "Stepwise Fwd: Number of Variables"), BICs_plot(regfit.fwd.summary, "Stepwise Fwd: Number of Variables"), nrow=2)
```

**Answer for Q8d - Forward StepWise Selection: **
The best models identified with forward step-wise are:
<ul>
<li>Cp: $Y = Intercept + X + X^2 + X^3 + X^4$</li>
<li>BIC: $Y = Intercept + X^3$</li>
<li>Adj R^2: $Y = Intercept + X + X^2 + X^3 + X^4$</li>
</ul>

Using forward step-wise selection, the best Adjusted R^2 Model is now the same as the best Mallow's Cp model which was not true with the regsubsets.

#### Backward
``` {r q8d-2}
regfit.bwd <- regsubsets(y~., data=df.q8c, nvmax=10, method="backward")
#summary(regfit.bwd)
regfit.bwd.sumary = regfit.bwd %>% broom::tidy()

regfit.bwd.sumary

grid.arrange(adjR2s_plot(regfit.bwd.sumary, "Stepwise Bwd: Number of Variables"), mallows_cps_plot(regfit.bwd.sumary, "Stepwise Bwd: Number of Variables"), BICs_plot(regfit.bwd.sumary, "Stepwise Bwd: Number of Variables"), nrow=2)
```

**Answer for Q8d - Backward StepWise Selection: **
The best models identified with backward step-wise are:
<ul>
<li>Cp: $Y = Intercept + X + X^2 + X^3$</li>
<li>BIC: $Y = Intercept + X^3$</li>
<li>Adj R^2: $Y = Intercept + X + X^2 + X^3 + X^7 + X^9 + X^{10}$</li>
</ul>

Using backward step-wise selection, all three measures produce different models. The best BIC model has been the same for all three subset selection methods. Adj-R2 and Cp measures select different models when the three selection methods are used.

It is clear that BOC places the greatest penalty on increasing the number of predictors as BIC always recommends the most minimal models when comparing to other measures of fit.

## Q8 (e) 
Now fit a lasso model to the simulated data, again using X,X2,. . . , X10 as predictors. 

Use cross-validation to select the optimal value of λ. 

Create plots of the cross-validation error as a function of λ. 

Report the resulting coefficient estimates, and discuss the results obtained.

```{r q8e-1}
library(caret)
set.seed(19)
# alpha 1 == lasso
lassoCV = caret::train(y~., data=df.q8c, method="glmnet", 
             tuneGrid=data.frame(alpha=rep(1,100), lambda=10^seq(1,-2,length=100)),
             trControl=trainControl("cv", number=10, returnResamp='all'))

lassoCV$results %>% ggplot(aes(seq_along(RMSE), RMSE)) + geom_line(size=1, color='red')

coef(lassoCV$finalModel, lassoCV$bestTune$lambda)

print(paste("Best Lambda with CV: ",lassoCV$bestTune$lambda))
```
**Answer Q8e:** Using cross-validation with lasso, the optimal value of lambda was **0.23**, and the best model, based on RMSE, is:
$$y = \beta0 + \beta1X + \beta2X^2 + \beta3X^3 + \beta4X^4$$
where:<br />
<ul>
<li>B0 = 10.28</li>
<li>B1 = -12.6</li>
<li>B2 = 2.75</li>
<li>B3 = -9.04</li>
<li>B4 = 0.08</li>
</ul>

These values **compare well** to the values with which I created the data set: <br />
<ul>
<li>B0 = 10</li>
<li>B1 = -13</li>
<li>B2 = 3.27</li>
<li>B3 = -9.11</li>
</ul>

## Q8 (f) 
Now generate a response vector Y according to the model:

Y = β0 + β7X7 + E, and perform best subset selection and the lasso. 

Discuss the results obtained.

```{r q8f-1}

# generate the new vector
B7 = 9.04
Y.Q8f = B0 + (B7 * X^7) + E
df.q8f = data.frame(x1 = X, x2 = X^2, x3 = X^3, x4 = X^4, x5 = X^5, x6 = X^6, x7 = X^7, x8 = X^8, x9 = X^9, x10 = X^10, y=Y.Q8f)

# regsubsets model
regfit.full.q8f <- leaps::regsubsets(y~., df.q8f)
reg.summary.q8f = regfit.full.q8f %>% broom::tidy() 
reg.summary.q8f
# show BIC coefficients
coef(regfit.full.q8f,1) %>% broom::tidy()

grid.arrange(adjR2s_plot(reg.summary.q8f, "Q8f RegSubSets: Number of Variables"), mallows_cps_plot(reg.summary.q8f, "Q8f RegSubSets: Number of Variables"), BICs_plot(reg.summary.q8f, "Q8f RegSubSets: Number of Variables"), nrow=2)


# fwd stepwise
regfit.fwd.q8f <- regsubsets(y~., data=df.q8f, nvmax=10, method="forward")
regfit.fwd.summary.q8f = regfit.fwd.q8f %>% broom::tidy()
regfit.fwd.summary.q8f
# show BIC coefficients
coef(regfit.fwd.q8f,1) %>% broom::tidy()

grid.arrange(adjR2s_plot(regfit.fwd.summary.q8f, " Q8f Stepwise Fwd: Number of Variables"), mallows_cps_plot(regfit.fwd.summary.q8f, "Q8f Stepwise Fwd: Number of Variables"), BICs_plot(regfit.fwd.summary.q8f, "Q8f Stepwise Fwd: Number of Variables"), nrow=2)

# bwd stepwise
regfit.bwd.q8f <- regsubsets(y~., data=df.q8f, nvmax=10, method="backward")
regfit.bwd.summary.q8f = regfit.bwd.q8f %>% broom::tidy()
regfit.bwd.summary.q8f
# show BIC coefficients
coef(regfit.bwd.q8f,1) %>% broom::tidy()

grid.arrange(adjR2s_plot(regfit.bwd.summary.q8f, " Q8f Stepwise Bwd: Number of Variables"), mallows_cps_plot(regfit.bwd.summary.q8f, "Q8f Stepwise Bwd: Number of Variables"), BICs_plot(regfit.bwd.summary.q8f, "Q8f Stepwise Bwd: Number of Variables"), nrow=2)

# lasso
set.seed(1976)
# alpha 1 == lasso
lassoCV.q8f = caret::train(y~., data=df.q8f, method="glmnet", 
             tuneGrid=data.frame(alpha=rep(1,100), lambda=10^seq(1,-2,length=100)),
             trControl=trainControl("cv", number=10, returnResamp='all'))

lassoCV.q8f$results %>% ggplot(aes(seq_along(RMSE), RMSE)) + geom_line(size=1, color='red')

coef(lassoCV.q8f$finalModel, lassoCV.q8f$bestTune$lambda)

lassoCV.q8f$bestTune$lambda

```
**Answer Q8f:**<br />

The **true** model is:
$$y = 10 + (9.04 * X^7) + E$$

Using **RegSubSets** the best models are:
<ul>
<li>Cp: $Y = Intercept + X^6 + X^7 + X^8 + X^{10}$</li>
<li>BIC: $Y = Intercept + X^7$</li>
<li>Adj R^2 produces a tie between many models, one includes the same model as identified with Cp: $Y = Intercept + X^6 + X^7 + X^8 + X^{10}$</li>
</ul>


Using **Fwd Selection** the best models are:
<ul>
<li>Cp: $Y = Intercept + X^2 + X^7 + X^8$</li>
<li>BIC: $Y = Intercept + X^7$</li>
<li>Adj R^2 produces a tie between many models, one includes the same model as identified with Cp: $Y = Intercept + X^2 + X^7 + X^8$</li>
</ul>


Using **Bwd Selection** the best models are:
<ul>
<li>Cp: $Y = Intercept + X^5 + X^7 + X^9$</li>
<li>BIC: $Y = Intercept + X^7$</li>
<li>Adj R^2 produces a tie between many models, the most minimal of the ties is: $Y = Intercept + X^3 + X^5 + X^7 + X^9$</li>
</ul>

The function with regsubsets, fwd selection, and bwd selection estimated coefficients as identified in the best BIC model are:
$$y = 9.95 + (9.04 * X^7)$$
This is **very close** to the true model.


Using the **lasso** the model identified uses lambda of **8.1** and is defined:
$$y = 7.78 + (8.78 * X^7)$$
The lasso did not get as close to the true values as the subset selection methods did.

# Question 9
In this exercise, we will predict the number of applications received using the other variables in the College data set.

## Q9 (a) 
Split the data set into a training set and a test set.
```{r q9a-1}
library(ISLR)

head(College)
#dim(College)
```
```{r q9a-2}
library(caret)

set.seed(1976)
college = sample_n(College, nrow(College))
train_indices <- sample(1:nrow(College), nrow(College) * .75)

college_train = College[train_indices,]
college_test = College[-train_indices,]
```

## Q9 (b) 
Fit a linear model using least squares on the training set, and report the test error obtained.
```{r q9b-1}
model.lm = lm(Apps ~ ., data = college_train)
broom::glance(model.lm)

```
```{r q9b-2}
model.glm_predictions = broom::augment(model.lm, newdata=college_test) 

#model.glm_predictions
mean((model.glm_predictions$Apps-model.glm_predictions$.fitted)^2)
```


## Q9 (c) 
Fit a ridge regression model on the training set, with λ chosen by cross-validation. 

Report the test error obtained.

**Here is the training**
```{r q9c-1}
# ridge
set.seed(1976)
# alpha 0 == ridge
ridge.q9c = caret::train(Apps ~ ., data=college_train, method="glmnet", 
             tuneGrid=data.frame(alpha=rep(0,100), lambda=10^seq(5,-4,length=100)),
             trControl=trainControl("cv", number=10, returnResamp='all'))

ridge.q9c$results %>% ggplot(aes(seq_along(RMSE), RMSE)) + geom_line(size=1, color='red')

coef(ridge.q9c$finalModel, ridge.q9c$bestTune$lambda)

ridge.q9c$bestTune$lambda

```

**Here is the test**
```{r q9c-2}
q9c.pred = predict(ridge.q9c, newdata=college_test)

# print the MSE for the predictions
mean((college_test$Apps-q9c.pred)^2)

# this would print the final model
#ridge.q9c$finalModel %>% summary()

#https://stackoverflow.com/questions/40493412/how-to-pick-a-different-model-from-the-finalmodel-in-caret
# if I wanted a different model from the one selected:
#ridge.q9c <- update(ridge.q9c, param = list(ncomp = 7))
#ridge.q9c$finalModel %>% summary()
```
**Observation Q9c:** the ***test MSE*** for the ridge regression is slightly lower than the linear model.
<ul>
<li>lm: 879419.2</li>
<li>ridge model: 860609.3</li>
</ul>

## Q9 (d) 
Fit a lasso model on the training set, with λ chosen by cross-validation.

Report the test error obtained, along with the number of non-zero coefficient estimates.

**Here is the training**
```{r q9d-1}
# lasso
set.seed(1976)
# alpha 1 == lasso
lasso.q9 = caret::train(Apps ~ ., data=college_train, method="glmnet", 
             tuneGrid=data.frame(alpha=rep(1,100), lambda=10^seq(5,-4,length=100)),
             trControl=trainControl("cv", number=10, returnResamp='all'))

lasso.q9$results %>% ggplot(aes(seq_along(RMSE), RMSE)) + geom_line(size=1, color='red')

coef(lasso.q9$finalModel, lasso.q9$bestTune$lambda)

lasso.q9$bestTune$lambda
```

**Here is the test**

```{r q9d-2}
q9d.pred = predict(lasso.q9, newdata=college_test)

# print the MSE for the predictions
mean((college_test$Apps-q9d.pred)^2)

```

**Observations Q9d:** The MSE for the lasso model is much less than the linear model, and less than the ridge model:
<ul>
<li>lm: 879419.2</li>
<li>ridge model: 860609.3</li>
<li>lasso model: 809997.9</li>
</ul>

And, of the predictors, there are ***11 with non-zero coefficients*** using the lasso:
<ol>
<li>PrivateYes: -442.87727790</li>
<li>Accept: 1.48335384</li>
<li>Enroll: -0.24851834</li>
<li>Top10perc: 29.75710945</li>
<li>Top25perc: -0.17766001</li>
<li>Outstate: -0.04732921</li>
<li>Room.Board: 0.08992306</li>
<li>PhD: -1.65337906</li>
<li>Terminal: -5.31239943</li>
<li>Expend: 0.06544648</li>
<li>Grad.Rate: 4.88401358</li>
</ol>

## Q9 (e) 
Fit a PCR model on the training set, with M chosen by cross-validation.

Report the test error obtained, along with the value of M selected by cross-validation.

```{r q9e-1}
set.seed(1976)

# use the pcr model - requires standardization: for 17 parameters
q9epcr.fits = train(Apps~., data=college_train, method="pcr", tuneLength=16, scale=TRUE, trControl=trainControl("cv", number=10, returnResamp='all'))

# plot the MSE as
q9epcr.fits$results %>% dplyr::mutate(MSE=RMSE^2) %>% ggplot(aes(seq_along(MSE), MSE)) + geom_line(size=2, color='red')

q9epcr.fits$resample %>% 
  dplyr::mutate(MSE=RMSE^2) %>%
  dplyr::group_split(Resample) %>% 
  dplyr::bind_rows() %>%
  ggplot(aes(ncomp, MSE)) + geom_point() +
  geom_line(q9epcr.fits$results%>%dplyr::mutate(MSE=RMSE^2), 
            mapping=aes(seq_along(MSE), MSE),
            size=2, color='red')

summary(q9epcr.fits)

```

```{r q9e-2}

# get the predictions
q9epcr.pred <- predict(q9epcr.fits, newdata=college_test)

mean((college_test$Apps-q9epcr.pred)^2)

q9epcr.fits$finalModel %>% summary()

```

**Observations Q9e:** The MSE for the PCR model where cross-validation selected M = 16 is between the ridge and lasso models.
<ul>
<li>lm: 879419.2</li>
<li>ridge model: 860609.3</li>
<li>lasso model: 809997.9</li>
<li>PCR | M == 16: 843803.1</li>
</ul>

## Q9 (f) 
Fit a PLS (Partial Least Squares) model on the training set, with M chosen by cross-validation.

Report the test error obtained, along with the value of M selected by cross-validation.

```{r q9f-1}
set.seed(1976)

q9fpls.fits = train(Apps~., data=college_train, method="pls", tuneLength=17, scale=TRUE, trControl=trainControl("cv", number=10, returnResamp='all'))

#summary(pls.fit)
q9fpls.fits$finalModel %>% summary()
```
```{r q9f-2}
q9fpls.fits$results %>% dplyr::mutate(MSE=RMSE^2) %>% ggplot(aes(seq_along(MSE), MSE)) + geom_line(size=2, color='red')

q9fpls.fits$resample %>% 
  dplyr::mutate(MSE=RMSE^2) %>%
  dplyr::group_split(Resample) %>% 
  dplyr::bind_rows() %>%
  ggplot(aes(ncomp, MSE)) + geom_point() +
  geom_line(q9fpls.fits$results%>%dplyr::mutate(MSE=RMSE^2), 
            mapping=aes(seq_along(MSE), MSE),
            size=2, color='red')
```

```{r q9f-3}
q9fpls.pred = predict(q9fpls.fits, newdata=college_test)
#mean((pls.pred-y.test)^2)
mean((college_test$Apps-q9fpls.pred)^2)

q9fpls.fits$finalModel %>% summary()
```
**Observations Q9f:** The MSE for the PLS model where cross-validation selected M = 12 is only slightly better performing on the test data than the linear model.
<ul>
<li>lm: 879419.2</li>
<li>ridge model: 860609.3</li>
<li>lasso model: 809997.9</li>
<li>PCR | M == 16: 843803.1</li>
<li>PLS | M == 12: 876331.9</li>
</ul>

## Q9 (g) 
Comment on the results obtained. How accurately can we predict the number of college applications received? Is there much difference among the test errors resulting from these five approaches?

```{r q9g-1}
lasso.tibble = as_tibble(lasso.q9$results)

lasso.tibble %>% filter(RMSE == min(RMSE))

```

```{r}
#lasso.q9$finalModel %>% summary()

#lasso.q9$finalModel$tuneValue

calc.RSquare = function(actual,predict){
  cor(actual,predict)^2
}

calc.RSquare(college_test$Apps, q9d.pred)

```

**Answer Q9g: ** The best performing model using the measure of MSE is the lasso model, which has an R Squared value of .937. This tells use that the model selected using a lambda of 28.5 and using the following 11 predictors predicts 93.7% of the variance in the number of applications to the universities in the sample.

<ol>
<li>PrivateYes: -442.87727790</li>
<li>Accept: 1.48335384</li>
<li>Enroll: -0.24851834</li>
<li>Top10perc: 29.75710945</li>
<li>Top25perc: -0.17766001</li>
<li>Outstate: -0.04732921</li>
<li>Room.Board: 0.08992306</li>
<li>PhD: -1.65337906</li>
<li>Terminal: -5.31239943</li>
<li>Expend: 0.06544648</li>
<li>Grad.Rate: 4.88401358</li>
</ol>
