---
title: "Module 8 HW McSpadden_Diana"
author: "Diana McSpadden"
date: "4/1/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Q7
In the lab, we applied random forests to the Boston data using mtry=6 and using ntree=25 and ntree=500. Create a plot displaying the test error resulting from random forests on this data set for a more comprehensive range of values for mtry and ntree. You can model your plot after Figure 8.10. Describe the results obtained.

```{r q7-setup, warning=FALSE}
library(MASS)
library(tree)
library(tidyverse)
library(partykit)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
library(randomForest)
```

Setup train and test observations
```{r q7-1}

boston <- MASS::Boston
set.seed(1976)

n = nrow(boston)


train = sample(1: nrow(boston ), 406, replace = FALSE) # 406 training observations
```

The argument **mtry=13** indicates that all 13 predictors should be considered for each split of the tree. 

I want to run for m = range(1:13).

We change the number of trees grown by randomForest() using the **ntree** argument.

I want to run with ntree in seq(from = 25, to = 500, by = 25)
```{r q7-2, cache=TRUE}

out_trees = tibble()

for (m in c(1,4,7,10,13))
{
  for (nt in seq(from = 1, to = 1000, by = 25))
  {
    set.seed(200)
    tree_model = randomForest(medvâˆ¼., data = boston, subset=train, mtry = m, ntree = nt)
    preds = predict(tree_model, newdata = boston[-train,])
    preds_err = mean((preds - boston[-train, ]$medv)^2)
    
    # save the tuning and errors
    out_trees = bind_rows(out_trees, tibble(mtry = m, ntree = nt, err = preds_err))
  }
}
```

``` {r q7-3}
# can save mtry as a factor and then the plot will color correctly
out_trees %>% mutate(mtry = factor(mtry)) %>% filter(err == min(err))

```

**My minimum error randomForrest used mtry = 10 and ntree = 26.**


Now I will create a plot:
```{r q7-4}

out_trees %>% ggplot(aes(x = ntree, y = err, color=factor(mtry))) + geom_line() #+ geom_point(aes(colour = factor(mtry))) #


```
**Answer Q7:** One can see that the mtry = 1 performed poorly no matter the number of trees; it is expected that using/comparing single predictor at each split would not perform well. One can see that 10 predictors was the best balance of bias and variance when using random forests, and that 26 trees was adequate to produce the lowest error rate.