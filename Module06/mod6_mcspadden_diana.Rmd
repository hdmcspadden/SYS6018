---
title: "mod6_mcspadden_diana"
author: "Diana McSpadden"
date: "3/23/2021"
output: html_document
---

# Question 6 
In this exercise, you will further analyze the Wage data set considered throughout this chapter.

```{r q6-setup, warning=FALSE}
library(ISLR)
library(tidyverse)
library(boot)

wage = ISLR::Wage %>% as_tibble()
```


##  Q6a) Perform polynomial regression to predict wage using age. 
Use cross-validation to select the optimal degree d for the polynomial.

**Which Cross-Validation**
```{r q6a-1}
cv.error = rep (0,10)

for (i in 1:10) {
  set.seed(1976)
  glm.fit = glm(wage ∼ poly(age ,i), data=wage)
  cv.error[i] = cv.glm(Wage ,glm.fit)$delta[1]
}

as.matrix(cv.error)

```
```{r q6a-2}
fit.1 = lm(wage∼age ,data=Wage)
fit.2 = lm(wage∼poly(age ,2) ,data=Wage)
fit.3 = lm(wage∼poly(age ,3) ,data=Wage)
fit.4 = lm(wage∼poly(age ,4) ,data=Wage)
fit.5 = lm(wage∼poly(age ,5) ,data=Wage)
fit.6 = lm(wage∼poly(age ,6) ,data=Wage)
fit.7 = lm(wage∼poly(age ,7) ,data=Wage)
fit.8 = lm(wage∼poly(age ,8) ,data=Wage)
fit.9 = lm(wage∼poly(age ,9) ,data=Wage)
fit.10 = lm(wage∼poly(age ,10) ,data=Wage)

anova(fit.1, fit.2, fit.3, fit.4, fit.5, fit.6, fit.7, fit.8, fit.9, fit.10)
```
What degree was chosen, and how does this compare to the results of hypothesis testing using ANOVA? Make a plot of the resulting polynomial fit to the data.

**Answer Q6a: Which Cross-Validation**: Cross-validation of poly models between 1 and 10 selected the degree-9 polynomial by error; however the degree-4 may also be a rational choice if we need to minimize flexibility/variance.

Using ANOVA either the cubic or degree-9 polynomial provide a good fit.

**Plot of resulting polynomial fit**:
<br />
I plotted both the degreee4 and degree 9.
```{r q6a-3}


ggplot() + geom_jitter(wage, mapping=aes(x=age, y=wage), size = 0.5, color='green') +
  stat_smooth(wage, mapping=aes(x=age, y=wage), method='lm', formula='y~poly(x,4)', se=TRUE, alpha=1) + 
  ggtitle("Degree 4 Polynomial")

ggplot() + geom_jitter(wage, mapping=aes(x=age, y=wage), size = 0.5, color='blue') +
  stat_smooth(wage, mapping=aes(x=age, y=wage), method='lm', formula='y~poly(x,9)', se=TRUE, alpha=1) + 
  ggtitle("Degree 9 Polynomial")
```
The plot fits reasonably well, with a little extra variance at the greater extreme end of age, which is natural because it is the extreme end of a polynomial and because there is sparse data at the greater end of the age range.

## Q6b) Fit a step function to predict wage using age, and perform crossvalidation ...
to choose the optimal number of cuts. 

``` {r q6battach, warning=FALSE, echo=FALSE}
attach(Wage)
```
Read <https://www.rdocumentation.org/packages/boot/versions/1.3-27/topics/cv.glm> to better understand what $delta is returning.
```{r q6b-1}
library(stringr)
out_step = tibble()


for (i in 6:15){
  
  set.seed(1976)
  
  cuts = levels(cut(age, i))
  #print(cuts)
  breaks = unique(c(as.numeric(sub("\\((.+),.*", "\\1", cuts)),
                    as.numeric(sub("[^,]*,([^]]*)\\]", "\\1", cuts))))
  step.fit = glm(wage~cut(age,unique(breaks)), data = Wage)
  err.step =  cv.glm(Wage ,step.fit, K=10)$delta[1]
   
  out_step = bind_rows(out_step, tibble(numCuts = i, cvError = err.step, cuts = str_flatten(cuts, "-")))
}

out_step

```
**Answer Q6b NUmber Cuts:** The optimal number of cuts, calculated by cross validation is **11**, although **8** is also reasonable.

Make a plot of the fit obtained.

```{r q6b-2}

wage <- ISLR::Wage %>% as_tibble()

ggplot() + geom_jitter(wage, mapping=aes(x=age, y=wage), color='green', size=0.5) +
  stat_smooth(wage, mapping=aes(x=age, y=wage), method='lm', formula='y~cut(x,11)', se=TRUE, alpha=1)
```
# Question 9
This question uses the variables dis (the weighted mean of distances to five Boston employment centers) and nox (nitrogen oxides concentration in parts per 10 million) from the Boston data. We will treat dis as the predictor and nox as the response.
```{r q9-setup-1}
detach(Wage)
```


```{r q9-setup-2}
library(MASS)
head(Boston)
```

## (a) Use the poly() function...
to fit a cubic polynomial regression to predict nox using dis. Report the regression output, and plot the resulting data and polynomial fits.

```{r q9a-1}
fit.q9.3 = lm(nox∼poly(dis ,3) ,data=Boston)

summary(fit.q9.3)

boston = MASS::Boston %>% as_tibble()

ggplot() + geom_jitter(boston, mapping=aes(x=dis, y=nox), size = 0.5, color= 'black') +
  stat_smooth(boston, mapping=aes(x=dis, y=nox), method='lm', formula='y~poly(x,3)', se=TRUE, alpha=1)
```

There is very sparse data at the greater extreme end of dis, and the cubic polynomial has significant variance when dis is > 8ish.

## (b) Plot the polynomial fits ...
for a range of different polynomial degrees (say, from 1 to 10), and report the associated residual sum of squares.

```{r q9b-1, fig.height=30, fig.width=10}
poly_plot = function(i) {
  ggplot() + geom_jitter(boston, mapping = aes(x=dis, y=nox), size = 0.5, color = 'black') +
  stat_smooth(boston, mapping=aes(x=dis, y=nox), method='lm', formula=(paste("y~poly(x,", i, ")")), se=TRUE, alpha=1) + 
  ggtitle(label = paste("nox ~ dis^", i))
}

library(gridExtra)

grid.arrange(poly_plot(1), poly_plot(2), poly_plot(3), poly_plot(4), poly_plot(5), poly_plot(6), poly_plot(7), poly_plot(8), poly_plot(9), poly_plot(10), nrow=5)
```
**Q9b Plot Observations:** as the polynomial increases I can see the increase in instability (i.e. the 95% confidence interval) in the extreme ends of the data.

```{r q9b-2}

poly.error = rep (0,9)

for (i in 1:10) {
  poly.fit = lm(nox ∼ poly(dis ,i), data=boston)
  poly.error[i] = sum(resid(poly.fit)^2)
}

as.matrix(poly.error)

```
**Q9b Observation:**
Not surprising that with the entire data set the residual sum squares decreases as the polynomial increases. I am guessing I am seeing over fitting.

## (c) Perform cross-validation or another approach ...
to select the optimal degree for the polynomial, and explain your results.

```{r q9c-1}
cv.9c.error = rep (0,10)

for (i in 1:10) {
  set.seed(1976)
  glm.fit = glm(nox ∼ poly(dis ,i), data=boston)
  cv.9c.error[i] = cv.glm(boston ,glm.fit)$delta[1]
}

as.matrix(cv.9c.error)
```

**Answer Q9c:** Based on cross-validation error the cubic polynomial performs the best. 

## (d) Use the bs() function to fit a regression spline...
to predict nox using dis. Report the output for the fit using four degrees of freedom. How did you choose the knots? Plot the resulting fit.

```{r q9d-setup}
library (splines)
```

```{r q9d-1}
fit.4df.spline = lm(nox∼bs(dis, df=4), data=boston)

attr(bs(boston$dis , df=4), "knots")

summary(fit.4df.spline)
```
**Answer Knots For DF=4 bs() spline:** I did not choose the knots, rather I let bs() select the knot for 4 degrees of freedom. THe knot was selected to be evenly distributed, so it is at the median value, 3.20745.


```{r q9d-2}
# function for plotting regression spline with number of dfs
bs_plot = function(dfs) {
  ggplot() + geom_jitter(boston, mapping=aes(x=dis, y=nox), size = 0.5, color = 'black') +
  stat_smooth(boston, mapping=aes(x=dis, y=nox), method='lm', formula=(paste("y~bs(x, df = ", dfs, ")")), se=TRUE, alpha=1) + 
  ggtitle(label = paste("nox ~ bs(dis, df =", dfs,")"))
}
```

```{r q9d-3}
bs_plot(4)
```

## (e) Now fit a regression spline ...
for a range of degrees of freedom, and plot the resulting fits and report the resulting RSS. 

```{r q9e-1, fig.width=10, fig.height=30}

grid.arrange(bs_plot(4), bs_plot(5), bs_plot(6), bs_plot(7), bs_plot(8), bs_plot(9), bs_plot(10), bs_plot(11), bs_plot(12), bs_plot(13), nrow=5)
```

```{r q9e-2}

bs.error = rep(0,13)

for (i in 1:13) {
  bs.fit = lm(nox ∼ bs(dis, df = i), data=boston)
  bs.error[i] = sum((resid(bs.fit))^2)
}

as.matrix(bs.error)
```
df's of 1,2, and 3 represent a linear model using B-splines (they are all df == 3), so their errors are all the same.

df == 8 is one "trough" in the collection of errors, that then decrease again starting at df = 10. I would guess that extreme over-fitting (i.e. high variance) would be a problem for models fitted with large df values.


##(f) Perform cross-validation or another approach...
in order to select the best degrees of freedom for a regression spline on this data.


I am going to try bootstrap to determine the best degrees of freedom for the regression spline.

```{r q9f-0, warning=FALSE}
library(tidymodels)
library(boot)

bs.error = rep (0,13)



for (dfs in 1:13)
{
  
  boot.fn=function (data, index)
  {
    #print(dfs)
    return (resid(glm(nox ∼ bs(dis, df = dfs)-1 ,data=data, subset = index)))
  }
  # set the seed in the loop so that each df loop uses the same splits
  set.seed(1976)
  
  bs.error[dfs] = sum(boot.fn(boston,sample(100,100,replace=T))^2)
}

as.matrix(bs.error)
```

Describe your results.

**Answer Q9f**

df's of 1,2, and 3 represent a linear model using B-splines (they are all df == 3), so their errors are all the same. Using Bootstrap df == 10, or 13 are the best fit based on cross validation error; however **df == 6** is a trough in the error values, and may be the best choice to balance bias and variance.


Df == 10 was selected both with the entire dataset and bootstrap validation.

# Question 10  

## (a) Split the data into a training set and a test set. 
Using out-of-state tuition as the response and the other variables as the predictors, perform forward stepwise selection on the training set in order to identify a satisfactory model that uses just a subset of the predictors.

```{r q10a-1}
library(ISLR)
college = ISLR::College %>% as_tibble(rownames = "College")
college = dplyr::select(college, -1) %>% mutate(Private = factor(Private))
head(college)

```

Split into training and test

```{r q10a-2}
set.seed(1976)

test = sample(nrow(college), 50) # set aside 50 records for testing
training = college[-test,]
```

```{r}
head(training)
```



```{r q10a-3}
library(leaps)

forward_model <- regsubsets(Outstate~., data=training, nvmax=17, method="forward") # allow a complete model to be used by setting nvmax to 17.
#summary(regfit.fwd)
forward_model %>% broom::tidy() %>% filter(BIC == min(BIC))

forward_model.summary = forward_model %>% broom::tidy()

#grid.arrange(adjR2s_plot(regfit.fwd.summary, "Stepwise Fwd: Number of Variables"), mallows_cps_plot(regfit.fwd.summary, "Stepwise Fwd: Number of Variables"), BICs_plot(regfit.fwd.summary, "Stepwise Fwd: Number of Variables"), nrow=2)
```

Based on best BIC we will use the model:

$$Outstate = Intercept + I(Private == Yes) + Apps + Accept + Top10perc + F.Undergrad + Room.Board + Personal + PhD + Terminal + S.F.Ratio + perc.alumni + Expend + Grad.Rate$$

## (b) Fit a GAM on the training data, ...
using out-of-state tuition as the response and the features selected in the previous step as the predictors. 

Plot the results, and explain your findings.

```{r q10b-1}
library(mgcv)

model_gam = mgcv::gam(Outstate ~ Private + s(Apps) + s(Accept) + s(Top10perc) + s(F.Undergrad) + s(Room.Board) + s(Personal) + s(PhD) + s(Terminal) + s(S.F.Ratio) + s(perc.alumni) + s(Expend) + s(Grad.Rate), data=training)
plot(model_gam, se=T, col="blue")

summary(model_gam)
```
**Answer q10b:**

Reviewing the summary() output from the mgcv gam model, the Private predictor is categorical and fitted with a logistic/catgorical model function.

The other predictors are fit with models with varied flexibility. From least to most flexible fit (based on degrees freedom assigned) the predictors are:
<ul>
<li>**Terminal** - df == 1: This fit is determined to be linear by the gam model. And the plot appears linear.</li>
<li>**Room.Board** - df == 1:  This fit is determined to be linear by the gam model. And the plot appears linear.</li>
<li>**Top10perc** - df == 1:  This fit is determined to be linear by the gam model. And the plot appears linear.</li>
<li>**perc.alumni** - df == 2.63:  This fit is determined to be between degree 2 and 3 by the gam model. The scale of the plot makes the relationship look near linear even with this flexibility.</li>
<li>**Personal** - df == 3.3: The fit is determined to be near cubic. The scale of the plot makes the relationship look near linear (a bit of wiggle but not much) even with this flexibility. </li>
<li>**PhD** - df == ~4: The near degree 4 fit shows a bit of wiggle in the plot. </li>
<li>**Expend** - df == 4.6: Some variance at the extremes of the plot are starting to become apparent.</li>
<li>**S.F.Ratio** - df == 5.2: Variance at the extreme ends (especially the greater end of S.F. Ratio) is becoming apparent as the flexibility of the fit is increasing.</li>
<li>**Grad.Rate** - df == 5.8: From the plot the flexible fit appears pretty good except for the variance at the far extremes.</li>
<li>**Accept** - df == 6: This has huge variance at the extremes, especially the greater end of the range of Accept.</li>
<li>**Apps** - df == 7: This has huge variance at the extremes, especially the greater end of the range of Apps</li>
<li>**F.Undergrad** - df == 8.6: Even with the high degree of flexibility (df == 8.6) the plot demonstrates a fairly decent fit, even at the extreme ends.</li>
</ul>

## (c) Evaluate the model obtained on the test set, ...
and explain the results obtained.

**Reference: <https://rdrr.io/cran/mgcv/man/predict.gam.html>**

```{r include=FALSE}
head(college[test,])
```

Here I calculate the MSE for the gam model
```{r q10-c-setup}
library(yardstick)
```

```{r q10c-1}

preds = predict(model_gam, college[test,], type = "response", se = TRUE)

yardstick::rmse_vec(college[test,]$Apps, preds$fit)

```
I would like to calculate the MSE of this model using a MLR to get some perspective on what I am seeing.

```{r q10c-2}
model_lm = lm(data=training, Outstate ~ Private + Apps + Accept + Top10perc + F.Undergrad + Room.Board + Personal + PhD + Terminal + S.F.Ratio + perc.alumni + Expend + Grad.Rate)

lm_pred = predict(model_lm, college[test,], type = "response", se = TRUE)

lm_mse = yardstick::rmse_vec(college[test,]$Apps, lm_pred$fit)

lm_mse
```
**Answer Q10c:** I am wondering if RMSE would be less using test data if some of the predictors did not have the increased flexibility of the s() function in the gam model

I would like to try the gam model again, with fewer s() fit predictors.
<ul>
<li>model_gam_2: keeps Apps, Accept, Room.Board, Terminal linear. I selected these because either they were identified as linear by s() in the gam model, or their high variance at extremes made me suspicious of over-fitting.</li>
<li>model_gam_3: I also made perc.alumni linear because it had the next lowest needed degrees freedom.</li>
<li>model_gam_4: I also made S.F.Ratio linear based on variance in plot at extreme ends.</li>
</ul>


```{r q10c-3}
model_gam_2 = mgcv::gam(Outstate ~ Private + Apps + Accept + s(Top10perc) + s(F.Undergrad) + Room.Board + s(Personal) + s(PhD) + Terminal + s(S.F.Ratio) + s(perc.alumni) + s(Expend) + s(Grad.Rate), data=training)

model_gam_3 = mgcv::gam(Outstate ~ Private + Apps + Accept + s(Top10perc) + s(F.Undergrad) + Room.Board + s(Personal) + s(PhD) + Terminal + s(S.F.Ratio) + perc.alumni + s(Expend) + s(Grad.Rate), data=training)

model_gam_4 = mgcv::gam(Outstate ~ Private + Apps + Accept + s(Top10perc) + s(F.Undergrad) + Room.Board + s(Personal) + s(PhD) + Terminal + S.F.Ratio + perc.alumni + s(Expend) + s(Grad.Rate), data=training)


# first gam model
gam_pred = predict(model_gam, college[test,], type = "response", se = TRUE)
mse_gam = yardstick::rmse_vec(college[test,]$Apps, gam_pred$fit)

# gam_2 model
gam_2_pred = predict(model_gam_2, college[test,], type = "response", se = TRUE)
mse_gam_2 = yardstick::rmse_vec(college[test,]$Apps, gam_2_pred$fit)

# gam_3 model
gam_3_pred = predict(model_gam_3, college[test,], type = "response", se = TRUE)
mse_gam_3 = yardstick::rmse_vec(college[test,]$Apps, gam_3_pred$fit)

# gam_4 model
gam_4_pred = predict(model_gam_4, college[test,], type = "response", se = TRUE)
mse_gam_4 = yardstick::rmse_vec(college[test,]$Apps, gam_4_pred$fit)


rMSE_out = tibble()

rMSE_out = bind_rows(rMSE_out, 
            tibble(modelname = "model_lm",
                   rMSE = lm_mse, 
                   smoothed_preds = ""))

rMSE_out = bind_rows(rMSE_out, 
            tibble(modelname = "model_gam",
                   rMSE = mse_gam, 
                   smoothed_preds = "Apps, Accept, Top10perc, F.Undergrad, Room.Board, Personal, PhD, Terminal, S.F.Ratio, perc.alumni, Expend, Grad.Rate"))
            
rMSE_out = bind_rows(rMSE_out, 
            tibble(modelname = "model_gam_2",
                  rMSE = mse_gam_2, 
                   smoothed_preds = "Top10perc, F.Undergrad, Personal, PhD, S.F.Ratio, perc.alumni, Expend, Grad.Rate"))
            
rMSE_out = bind_rows(rMSE_out, 
            tibble(modelname = "model_gam_3",
                  rMSE = mse_gam_3, 
                   smoothed_preds = "Top10perc, F.Undergrad, Personal, PhD, S.F.Ratio, Expend, Grad.Rate"))
            
rMSE_out = bind_rows(rMSE_out, 
            tibble(modelname = "model_gam_4",
                  rMSE = mse_gam_4, 
                   smoothed_preds = "Top10perc, F.Undergrad, Personal, PhD, Expend, Grad.Rate"))


rMSE_out
```



## (d) For which variables, if any, is there evidence of a non-linear
relationship with the response?

```{r q10d-1}
summary(model_gam_4)
```


**Answer Q10d:**
You can see that for **model_gam_4** some of the predictors were made linear by the s() gam function (observed by degrees freedom in final gam model): Top10perc, Grad.Rate.

I didn't run through all the combinations of s() on each of the 12 predictors, but when I used degrees freedom, and viewing the plots to decide which predictors to use a non-linear relationship with, the model with the least RMSE has non-linear relationships with these four predictors (when other selected predictors are in the model):

<ul>
<li>F.Undergrad</li>
<li>Personal</li>
<li>PhD</li>
<li>Expend</li>
</ul>
