---
title: "mod6_mcspadden_diana"
author: "Diana McSpadden"
date: "3/23/2021"
output: html_document
---

# Question 6 
In this exercise, you will further analyze the Wage data set considered throughout this chapter.

```{r q6-setup, warning=FALSE}
library(ISLR)
library(tidyverse)
library(boot)

wage = ISLR::Wage %>% as_tibble()
```


##  Q6a) Perform polynomial regression to predict wage using age. 
Use cross-validation to select the optimal degree d for the polynomial.

**Which Cross-Validation**
```{r q6a-1}
cv.error = rep (0,5)

for (i in 1:5) {
  glm.fit = glm(wage ∼ poly(age ,i), data=wage)
  cv.error[i] = cv.glm (Wage ,glm.fit)$delta[1]
}

as.matrix(cv.error)

```
```{r q6a-2}
fit.1 = lm(wage∼age ,data=Wage)
fit.2 = lm(wage∼poly(age ,2) ,data=Wage)
fit.3 = lm(wage∼poly(age ,3) ,data=Wage)
fit.4 = lm(wage∼poly(age ,4) ,data=Wage)
fit.5 = lm(wage∼poly(age ,5) ,data=Wage)

anova(fit.1, fit.2, fit.3, fit.4, fit.5)
```
What degree was chosen, and how does this compare to the results of hypothesis testing using ANOVA? Make a plot of the resulting polynomial fit to the data.

**Answer Q6a: Which Cross-Validation**: Cross-validation of poly models between 1 and 5 selected the degree-4 polynomial by error; however the cubic was nearly as good.

Using ANOVA either the cubic or degree-4 polynomial provide a good fit.

**Plot of resulting polynomial fit**:
<br />
```{r q6a-3}


ggplot() + geom_jitter(wage, mapping=aes(x=age, y=wage),
              shape='.', size=.1, color='green', height=2) +
  #geom_ribbon(data=prediction_interval, fill = "black", alpha = 0.2, mapping=aes(x=age, ymin=lower, ymax=upper)) + 
  stat_smooth(wage, mapping=aes(x=age, y=wage), method='lm', formula='y~poly(x,4)', se=TRUE, alpha=1) #+
  #geom_line(prediction_interval, mapping=aes(x=age, y=pred))
```


## Q6b) Fit a step function to predict wage using age, and perform crossvalidation ...
to choose the optimal number of cuts. 

``` {r q6battach, warning=FALSE, echo=FALSE}
attach(Wage)
```

```{r q6b-1}
library(stringr)

set.seed(1976)

out_step = tibble()


for (i in 6:15){
  cuts = levels(cut(age, i))
  #print(cuts)
  breaks = unique(c(as.numeric(sub("\\((.+),.*", "\\1", cuts)),
                    as.numeric(sub("[^,]*,([^]]*)\\]", "\\1", cuts))))
  step.fit = glm(wage~cut(age,unique(breaks)), data = Wage)
  err.step =  cv.glm(Wage ,step.fit, K=10)$delta[1]
   
  out_step = bind_rows(out_step, tibble(numCuts = i, cvError = err.step, cuts = str_flatten(cuts, "-")))
}

out_step

```
**Answer Q6b NUmber Cuts:** The optimal number of cuts, calculated by cross validation is **8**.


Make a plot of the fit obtained.

```{r q6b-2}

wage <- ISLR::Wage %>% as_tibble()

ggplot() + geom_jitter(wage, mapping=aes(x=age, y=wage), shape='.', size=.1, color='green', height=2) +
  #geom_ribbon(data=prediction_interval, fill = "black", alpha = 0.2, mapping=aes(x=age, ymin=lower, ymax=upper)) + 
  stat_smooth(wage, mapping=aes(x=age, y=wage), method='lm', formula='y~cut(x,8)', se=TRUE, alpha=1) #+
  #geom_line(prediction_interval, mapping=aes(x=age, y=pred))
```
# Question 9
This question uses the variables dis (the weighted mean of distances to five Boston employment centers) and nox (nitrogen oxides concentration in parts per 10 million) from the Boston data. We will treat dis as the predictor and nox as the response.
```{r q9-setup-1}
detach(Wage)
```


```{r q9-setup-2}
library(MASS)
head(Boston)
```

## (a) Use the poly() function...
to fit a cubic polynomial regression to predict nox using dis. Report the regression output, and plot the resulting data and polynomial fits.

```{r q9a-1}
fit.q9.3 = lm(nox∼poly(dis ,3) ,data=Boston)

summary(fit.q9.3)

boston = MASS::Boston %>% as_tibble()

ggplot() + geom_jitter(boston, mapping=aes(x=dis, y=nox), shape='.', size=1, color='black', height=2) +
  stat_smooth(boston, mapping=aes(x=dis, y=nox), method='lm', formula='y~poly(x,3)', se=TRUE, alpha=1)
```


## (b) Plot the polynomial fits ...
for a range of different polynomial degrees (say, from 1 to 10), and report the associated residual sum of squares.

```{r q9b-1, fig.height=30, fig.width=10}
poly_plot = function(i) {
  ggplot() + geom_jitter(boston, mapping=aes(x=dis, y=nox), shape='.', size=2, color='black', height=2) +
  stat_smooth(boston, mapping=aes(x=dis, y=nox), method='lm', formula=(paste("y~poly(x,", i, ")")), se=TRUE, alpha=1) + 
  ggtitle(label = paste("nox ~ dis^", i))
}

library(gridExtra)

grid.arrange(poly_plot(1), poly_plot(2), poly_plot(3), poly_plot(4), poly_plot(5), poly_plot(6), poly_plot(7), poly_plot(8), poly_plot(9), poly_plot(10), nrow=5)
```
**Q9b Plot Observations:** as the polynomial increases I can see the increase in instability (i.e. the 9%% confidence interval) in the extreme ends of the data.

```{r q9b-2}

poly.error = rep (0,9)

for (i in 1:10) {
  poly.fit = lm(nox ∼ poly(dis ,i), data=boston)
  poly.error[i] = sum(resid(poly.fit)^2)
}

as.matrix(poly.error)

```
**Q9b Observation:**
Not surprising that with the entire data set the residual sum squares decreases as the polynomial increases. I am guessing I am seeing over fitting.

## (c) Perform cross-validation or another approach ...
to select the optimal degree for the polynomial, and explain your results.

```{r q9c-1}
cv.9c.error = rep (0,10)

for (i in 1:10) {
  glm.fit = glm(nox ∼ poly(dis ,i), data=boston)
  cv.9c.error[i] = cv.glm(boston ,glm.fit)$delta[1]
}

as.matrix(cv.9c.error)
```

**Answer Q9c:** Based on cross-validation error the cubic polynomial performs the best. 

## (d) Use the bs() function to fit a regression spline...
to predict nox using dis. Report the output for the fit using four degrees of freedom. How did you choose the knots? Plot the resulting fit.

```{r q9d-setup}
library (splines)
```

```{r q9d-1}
fit.4df.spline = lm(nox∼bs(dis, df=4), data=boston)

attr(bs(boston$dis , df=4), "knots")

summary(fit.4df.spline)
```
**Answer Knots For DF=4 bs() spline:** I did not choose the knots, but let the bs function select the knot for 4 degrees of freedom. THe knot was selected to be evenly distributed, so it is at the median value, 3.20745.


```{r q9d-2}
# function for plotting regression spline with number of dfs
bs_plot = function(dfs) {
  ggplot() + geom_jitter(boston, mapping=aes(x=dis, y=nox), shape='.', size=1, color='black', height=1) +
  stat_smooth(boston, mapping=aes(x=dis, y=nox), method='lm', formula=(paste("y~bs(x, df=", dfs, ")")), se=TRUE, alpha=1) + 
  ggtitle(label = paste("nox ~ bs(dis, df=", dfs,")"))
}
```

```{r q9d-3}
bs_plot(4)
```

## (e) Now fit a regression spline ...
for a range of degrees of freedom, and plot the resulting fits and report the resulting RSS. 

```{r q9e-1, fig.width=10, fig.height=30}

grid.arrange(bs_plot(4), bs_plot(5), bs_plot(6), bs_plot(7), bs_plot(8), bs_plot(9), bs_plot(10), bs_plot(11), bs_plot(12), bs_plot(13), nrow=5)
```

```{r q9e-2}

bs.error = rep(0,13)

for (i in 4:13) {
  bs.fit = lm(nox ∼ bs(dis, df = i), data=boston)
  bs.error[i] = sum((resid(bs.fit))^2)
}

as.matrix(bs.error)
```
**Answer to "Describe the results obtained":**


##(f) Perform cross-validation or another approach...
in order to select the best degrees of freedom for a regression spline on this data.


I am going to try bootstrap to determine the best degrees of freedom for the regression spline.

```{r q9f-0}
library(tidymodels)
library(boot)

bs.error = rep (0,13)

for (dfs in 4:13)
{
  boot.fn=function (data, index)
    return (resid(glm(nox ∼ bs(dis, df = dfs)-1 ,data=data, subset = index)))
  
  set.seed(1976)
  
  bs.error[dfs] = sum(boot.fn(boston,sample(100,100,replace=T)))^2
}

as.matrix(bs.error)
```

Describe your results.

**Answer Q9f**

***I do not understand this***


# Question 10  

## (a) Split the data into a training set and a test set. 
Using out-of-state tuition as the response and the other variables as the predictors, perform forward stepwise selection on the training set in order to identify a satisfactory model that uses just a subset of the predictors.

```{r q10a-1}
library(ISLR)
college = ISLR::College %>% as_tibble(rownames = "College")
college = dplyr::select(college, -1)
head(college)

```

Split into training and test

```{r q10a-2}
set.seed(1976)

test = sample(nrow(college), 250) # set aside 250 records for testing
training = college[-test,]
```

```{r q10a-3}
library(leaps)

forward_model <- regsubsets(Outstate~., data=training, nvmax=17, method="forward")
#summary(regfit.fwd)
forward_model %>% broom::tidy() %>% filter(BIC == min(BIC))

forward_model.summary = forward_model %>% broom::tidy()

#grid.arrange(adjR2s_plot(regfit.fwd.summary, "Stepwise Fwd: Number of Variables"), mallows_cps_plot(regfit.fwd.summary, "Stepwise Fwd: Number of Variables"), BICs_plot(regfit.fwd.summary, "Stepwise Fwd: Number of Variables"), nrow=2)
```

Based on best BIC we will use the model:

$$Outstate = Intercept + I(Private == Yes) + Accept + Enroll + Room.Board + PhD + perc.alumni + Expend + Grad.Rate$$

## (b) Fit a GAM on the training data, ...
using out-of-state tuition as the response and the features selected in the previous step as the predictors. 

Plot the results, and explain your findings.

```{r q10b-1}
library(mgcv)

model_gam = mgcv::gam(Outstate ~ Private + s(Accept) + s(Enroll) + s(Room.Board) + s(PhD) + s(perc.alumni) + s(Expend) + s(Grad.Rate), data=training)
plot(model_gam, se=T, col="blue")

summary(model_gam)
```
**Answer q10b:**

Reviewing the summary() output from the mgcv gam model, the Private predictor is categorical and fitted with a logistic/catgorical model function.

The other predictors are fit with models with varied flexibility. From least to most flexible fit the predictors are:
<ul>
<li>**PhD** - based on the single degree of freedom, PhD is fit with a linear model. This is also clear by looking at the plot of the fit of Outstate~PhD|other predictors are held constant. </li>
<li>**perc.alumni** - is fit with an almost linear model (df == 1.3). The plot of Outstate~perc.alumni|other predictors held constant appears to be nearly a straight line.</li>
<li>**Grad.Rate** has 4.567 degree freedom and displays some wiggly-ness. From the plot one can infer there are a few "knots" or other features allowing flexibility in the slop of the relationship. The deviation from the fit of the se at the extreme ends makes me wonder if this was fit with a polynomial.</li>
<li>**Room.Board** has 5.997 degrees of freedom and the plot is slightly more wiggly than Grad.Rate, but with less se variance at the extreme ends than Grad.Rate.</li>
<li>**Accept** - has 6.348 degrees of freedom and has several "humps" in the plot. The fit of Outstate~Accept|other variables held constant has large se variance at the greater extreme end of Accept.</li>
<li>**Expend** - has 6.494 degrees of freedom. There are distinct "humps" in the plot. The fit's se variance increases as Expend increases (after Expend > 10000).</li>
<li>**Enroll** - required the greatest flexibility in fit and displays the greatest wiggly-ness and greatest se variance at the greater extreme end of Enroll.</li>
</ul>

## (c) Evaluate the model obtained on the test set, ...
and explain the results obtained.

**Reference: <https://rdrr.io/cran/mgcv/man/predict.gam.html>**

```{r }
head(college[test,])
```

Here I calculate the MSE for the gam model
```{r q10c-1}
preds = predict(model_gam, college[test,], type = "response", se = TRUE)

mean((college[test,]$Outstate - preds$fit)^2)

```

## (d) For which variables, if any, is there evidence of a non-linear
relationship with the response?

**Answer Q10c:**

Based on reviewing the output of the trained model the following predictors appear to be in a non-linear relationship with Outstate:

<ul>
<li>**Grad.Rate** has 4.567 degree freedom and displays some wiggly-ness. From the plot one can infer there are a few "knots" or other features allowing flexibility in the slop of the relationship. The deviation from the fit of the se at the extreme ends makes me wonder if this was fit with a polynomial.</li>
<li>**Room.Board** has 5.997 degrees of freedom and the plot is slightly more wiggly than Grad.Rate, but with less se variance at the extreme ends than Grad.Rate.</li>
<li>**Accept** - has 6.348 degrees of freedom and has several "humps" in the plot. The fit of Outstate~Accept|other variables held constant has large se variance at the greater extreme end of Accept.</li>
<li>**Expend** - has 6.494 degrees of freedom. There are distinct "humps" in the plot. The fit's se variance increases as Expend increases (after Expend > 10000).</li>
<li>**Enroll** - required the greatest flexibility in fit and displays the greatest wiggly-ness and greatest se variance at the greater extreme end of Enroll.</li>
</ul>