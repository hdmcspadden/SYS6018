---
title: "Chapter 10 Lab"
author: "Diana McSpadden"
date: "4/20/2021"
output: html_document
---

```{r fig.width=10, fig.height=8}
# Chapter 10 Lab 1: Principal Components Analysis

# show what states are in the dataset
states=row.names(USArrests)
states

# names of the feature variables
names(USArrests)

# the mean of Murder, Assault, UrbanPop, and Rape
apply(USArrests, 2, mean)
# the variance of Murder, Assault, UrbanPop, and Rape
apply(USArrests, 2, var)
```


```{r fig.width=10, fig.height=8}
# notice the scaling
pr.out=prcomp(USArrests, scale=TRUE) # scale by mean and standard deviation
names(pr.out)

pr.out$center
pr.out$scale
pr.out$rotation # matrix used to rotate the data
dim(pr.out$x)

# already scaled
biplot(pr.out, scale=0)

```

```{r fig.width=10, fig.height=8}
pr.out$rotation=-pr.out$rotation # rotate the plot
pr.out$x=-pr.out$x
biplot(pr.out, scale=0)
```


``` {r}
pr.out$sdev
pr.var=pr.out$sdev^2
pr.var

# compute amount of information contained in each component == the amount of variance explained in each PC
pve=pr.var/sum(pr.var)
pve # proportion of variance contained in each component

plot(pve, xlab="Principal Component", ylab="Proportion of Variance Explained", ylim=c(0,1),type='b')

plot(cumsum(pve), xlab="Principal Component", ylab="Cumulative Proportion of Variance Explained", ylim=c(0,1),type='b')

# this is how cumsum() works
a=c(1,2,8,-3)
cumsum(a)
```

# Chapter 10 Lab 2: Clustering

# K-Means Clustering

```{r}
set.seed(2)
# generate some data
x=matrix(rnorm(50*2), ncol=2)
x[1:25,1]=x[1:25,1]+3 # shift first 25 +3
x[1:25,2]=x[1:25,2]-4 # shift last 25 - 4

# kmeans with 2 clusters
## nstart == random starts do help here with too many clusters
## (and are often recommended anyway!):
km.out=kmeans(x,2,nstart=20)
km.out$cluster

plot(x, col=(km.out$cluster+1), main="K-Means Clustering Results with K=2", xlab="", ylab="", pch=20, cex=2)


set.seed(4)
# kmeans with 3 clusters
km.out=kmeans(x,3,nstart=20)
km.out
plot(x, col=(km.out$cluster+1), main="K-Means Clustering Results with K=3", xlab="", ylab="", pch=20, cex=2) # pch, and cex give size and type of points

# only one starting selection
set.seed(3)
km.out=kmeans(x,3,nstart=1)
km.out$tot.withinss # quality of the clustering

set.seed(3)
km.out=kmeans(x,3,nstart=20)
km.out$tot.withinss
```

# Hierarchical Clustering

``` {r fig.width=12, fig,height=10}
hc.complete=hclust(dist(x), method="complete") # complete method
hc.average=hclust(dist(x), method="average")
hc.single=hclust(dist(x), method="single")
par(mfrow=c(1,3))
plot(hc.complete,main="Complete Linkage", xlab="", sub="", cex=.9)
plot(hc.average, main="Average Linkage", xlab="", sub="", cex=.9)
plot(hc.single, main="Single Linkage", xlab="", sub="", cex=.9)
cutree(hc.complete, 2)
cutree(hc.average, 2)
cutree(hc.single, 2)
cutree(hc.single, 4)
```

```{r}
# scale the values
xsc=scale(x)
plot(hclust(dist(xsc), method="complete"), main="Hierarchical Clustering with Scaled Features")
x=matrix(rnorm(30*3), ncol=3)
dd=as.dist(1-cor(t(x)))
plot(hclust(dd, method="complete"), main="Complete Linkage with Correlation-Based Distance", xlab="", sub="")
```

# Chapter 10 Lab 3: NCI60 Data Example

# The NCI60 data

```{r}
library(ISLR)
nci.labs=NCI60$labs
nci.data=NCI60$data
dim(nci.data)
nci.labs[1:4]
table(nci.labs)
```


# PCA on the NCI60 Data

``` {r}
pr.out=prcomp(nci.data, scale=TRUE)
Cols=function(vec){
    cols=rainbow(length(unique(vec))) # generate the number of colors of the length of unique values in the vector
    return(cols[as.numeric(as.factor(vec))]) # return the values as numeric
  }
par(mfrow=c(1,2))
plot(pr.out$x[,1:2], col=Cols(nci.labs), pch=19,xlab="Z1",ylab="Z2")

plot(pr.out$x[,c(1,3)], col=Cols(nci.labs), pch=19,xlab="Z1",ylab="Z3")

summary(pr.out)

plot(pr.out)

pve=100*pr.out$sdev^2/sum(pr.out$sdev^2)
par(mfrow=c(1,2))
plot(pve,  type="o", ylab="PVE", xlab="Principal Component", col="blue")
plot(cumsum(pve), type="o", ylab="Cumulative PVE", xlab="Principal Component", col="brown3")
```

# Clustering the Observations of the NCI60 Data

``` {r fig.width=11, fig.height=9}
sd.data=scale(nci.data)
par(mfrow=c(1,3))
data.dist=dist(sd.data)
plot(hclust(data.dist), labels=nci.labs, main="Complete Linkage", xlab="", sub="",ylab="")
plot(hclust(data.dist, method="average"), labels=nci.labs, main="Average Linkage", xlab="", sub="",ylab="")
plot(hclust(data.dist, method="single"), labels=nci.labs,  main="Single Linkage", xlab="", sub="",ylab="")
hc.out=hclust(dist(sd.data))
hc.clusters=cutree(hc.out,4)
table(hc.clusters,nci.labs)
par(mfrow=c(1,1))
plot(hc.out, labels=nci.labs)
abline(h=139, col="red")
hc.out
set.seed(2)
km.out=kmeans(sd.data, 4, nstart=20)
km.clusters=km.out$cluster
table(km.clusters,hc.clusters)

# get first 5 score vectors
hc.out=hclust(dist(pr.out$x[,1:5]))
plot(hc.out, labels=nci.labs, main="Hier. Clust. on First Five Score Vectors")
table(cutree(hc.out,4), nci.labs)


```


