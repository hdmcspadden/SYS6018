---
title: "Disaster Relief Project: Part I"
author: "H. Diana McSpadden"
date: "`r format(Sys.Date(), '%b %d, %Y')`"
output:
  html_document:
    number_sections: true    
    toc: true
    toc_float: true
    theme: cosmo
    highlight: espresso    
# You can make the format personal - this will get you started:  
# https://bookdown.org/yihui/rmarkdown/html-document.html#appearance_and_style    
---

<!--- Below are global settings for knitr. You can override any of them by adding the changes to individual chunks --->

```{r global_options, include=FALSE}
knitr::opts_chunk$set(error=TRUE,        # Keep compiling upon error
                      collapse=FALSE,    # collapse by default
                      echo=TRUE,         # echo code by default
                      comment = "#>",    # change comment character
                      fig.width = 5.5,     # set figure width
                      fig.align = "center",# set figure position
                      out.width = "49%", # set width of displayed images
                      warning=TRUE,      # show R warnings
                      message=TRUE)      # show R messages
```

<!--- Change font sizes (or other css modifications) --->
<style>
h1.title {
  font-size: 2.2em; /* Title font size */
}
h1 {
  font-size: 2em;   /* Header 1 font size */
}
h2 {
  font-size: 1.5em;
}
h3 { 
  font-size: 1.2em;
}
pre {
  font-size: 0.8em;  /* Code and R output font size */
}
</style>



**SYS 6018 | Spring 2021 | University of Virginia **

*******************************************

# Introduction 

Tell the reader what this project is about. Motivation. 

# Training Data / EDA

Load data, explore data, etc. 
```{r echo=FALSE}
#install.packages("pROC")
#install.packages("randomForest")
#install.packages("reticulate")
#install("regclass")
```


```{r load-packages, warning=FALSE, message=FALSE}
# Load Required Packages
library(tidyverse)
library(pROC)
library(randomForest)
library("GGally")
library(gridExtra)
library(plotly)
library(reticulate)
library(regclass)

```





```{r load-data, message=FALSE, warning=FALSE}
url = 'HaitiPixels.csv'
#url = 'https://collab.its.virginia.edu/access/lessonbuilder/item/1707832/group/17f014a1-d43d-4c78-a5c6-698a9643404f/Module3/HaitiPixels.csv' #this url is beng 
haiti <- read_csv(url)
print(dim(haiti))

```
```{r data-head1}
head(haiti)
```

The dataframe contains 4 columns, and 63,241 rows. The Class column contains the correct label for the observation. Red, Green and Blue parameters are **NEED TO INCLUDE CORRECT DEFINITION**

## Class Factor
To prepare the data for exploratory data analysis I must make Class a factor.
```{r factorClass}
haiti %>% 
  mutate(Class = factor(Class)) 
```

```{r summarizeByCategory}
haiti %>%
  group_by(Class) %>%
  summarize(N = n()) %>%
  mutate(Perc = round(N / sum(N), 2) * 100)
```

The records are not evenly distributed between the categories. Of the Classes Blue Tarp, our "positive" category if we are thinking a binary positive/negative identification, is only 3% of our sample. Soil and Vegetation make up the majority of our sample at 74%.

## Binary Class Factor vs. 5 Class Factor
It will be interesting to see performance predicting each of these categories, or a binary is or is not Blue Tarp.

### Create Binary DataFrame

Create a DataFrame that is only Blue Tarp, or not Blue Tarp:
* 0 == Not a Blue Tarp
* 1 == Is a Blue Tarp

```{r createBinaryDataFrame}
haitiBinary <- haiti %>%
  mutate(ClassBinary = if_else(Class == 'Blue Tarp', '1', '0'), ClassBinary = factor(ClassBinary))

```

```{r summarizeBinaryByCategory}

haitiBinary %>%
  group_by(ClassBinary) %>%
  summarize(N = n()) %>%
  mutate(Perc = round(N / sum(N), 2) * 100)

```

### How are red, blue and green values distributed between the 5 categories?

```{r colorValuesFiveCats}

redplot <- ggplot(haiti, aes(x=Class, y=Red)) + 
  geom_boxplot(col='red')

greenplot <- ggplot(haiti, aes(x=Class, y=Green)) + 
  geom_boxplot(col='darkgreen')

blueplot <- ggplot(haiti, aes(x=Class, y=Blue)) + 
  geom_boxplot(col='darkblue')

grid.arrange(redplot, greenplot, blueplot)
```


### How are red, blue and green values distributed between the binary categories?

```{r colorValuesBinaryCats}

redplotB <- ggplot(haitiBinary, aes(x=ClassBinary, y=Red)) + 
  geom_boxplot(col='red')

greenplotB <- ggplot(haitiBinary, aes(x=ClassBinary, y=Green)) + 
  geom_boxplot(col='darkgreen')

blueplotB <- ggplot(haitiBinary, aes(x=ClassBinary, y=Blue)) + 
  geom_boxplot(col='darkblue')

grid.arrange(redplotB, greenplotB, blueplotB)
```
**Box Plot Comments**

"Blue Tarp" as the "positive" result, and other results as the "negative" result.

Regarding the box plot of the five categories, of interest is that "Soil" and "Vegetation" are relatively unique in their non-outlier RGB values. "Rooftop" and "Various Non-Tarp" are more similar in their RBG values.

If the classes are collapsed to binary values of "Blue Tarp (1)" and "Not Blue Tarp (0)" there is little overlap in the blue values for the two classes, and the ranges of red and green are much smaller for blue tarp than non-blue-tarp.

Generally, the values of red have a larger range for negative results than for positive results, and the positive results have a similar median to the negative results.
Green values have a larger range for negative results than for positive results, and the positive results have a higher median than the negative results, and there is ***almost*** **no overlap in the blue data** with non-blue tarps, and blue tarps.

### View the correlation between Red, Green and Blue

These correlations make sense as the pixels were of highly saturated colors, that are not pure Blue, Red or Green. There are few pixels in the data set with low values for R,G,B.

```{r }
ggpairs(haiti[-1], lower = list(continuous = "points", combo = "dot_no_facet"), progress = F)
```
### 3-D Scatterplot

To view the relationship between the Red, Green, and Blue values between the five classes, and the binary classes, an interactive 3-D scatter plot is extremely useful.

**References**
https://plotly.com/python/3d-scatter-plots/

https://plotly.com/r/figure-labels/

#### Five Categories 3-D Scatterplot
The scatter plot displays 

```{r 3D5Class}

fiveCat3D = plot_ly(x=haiti$Red, y=haiti$Blue, z=haiti$Green, type="scatter3d", mode="markers", color=haiti$Class, colors = c('blue2','azure4','chocolate4','coral2','chartreuse4'),
marker = list(symbol = 'circle', sizemode = 'diameter', opacity =0.35))

fiveCat3D = fiveCat3D %>%
  layout(title="5 Category RBG Plot", scene = list(xaxis = list(title = "Red", color="red"), yaxis = list(title = "Blue", color="blue"), zaxis = list(title = "Green", color="green")))

fiveCat3D

```
One can see that there are discernible groupings of pixel categories by RGB values. Unsurprisingly, the blue tarps are higher blue values, but they do have a range of red and green values.


The 3D scatter plot is particularly useful because, by zooming in, one can see that there is a space in the 3D plot with significant mingling of "blue tarp" pixels and other pixel categories. That area of the data will provide a challenge for our model.

```{r 3DBinary}
binary3D = plot_ly(x=haitiBinary$Red, y=haitiBinary$Blue, z=haitiBinary$Green, type="scatter3d", mode="markers", color=haitiBinary$ClassBinary, colors = c('red','blue2'),
marker = list(symbol = 'circle', sizemode = 'diameter', opacity =0.35))

binary3D = binary3D %>%
  layout(title="Binary RBG Plot", scene = list(xaxis = list(title = "Red", color="red"), yaxis = list(title = "Blue", color="blue"), zaxis = list(title = "Green", color="green")))

binary3D
```
**Comments**

Similar to the five category 3D scatter plot, the binary scatter plot shows distinct groupings for blue tarp and non-blue-tarp. As expected, there is mingling of blue tarp and non-blue-tarp pixels that will provide a challenge for a model.


# Model Training

## Set-up 

Normalization does not need to be considered because the ranges of Red, Green and Blue are the same.

### Training and Test Data

The DataFrame must be divided into training and test data sets; however, the data set is unbalanced with few "positive" results, i.e. "Blue Tarp", compared with negative results.

??? Use ROSE library:
References:

**caret**
https://www.rdocumentation.org/packages/caret/versions/6.0-86/topics/trainControl
http://www.edii.uclm.es/~useR-2013/Tutorials/kuhn/user_caret_2up.pdf
https://www.rdocumentation.org/packages/caret/versions/6.0-86/topics/train

**ROSE**
https://www.r-bloggers.com/2017/04/dealing-with-unbalanced-data-in-machine-learning/
https://journal.r-project.org/archive/2014-1/menardi-lunardon-torelli.pdf

If using ROSE library, the steps are:
1. Split data set into Q = n/k data sets (where k refers to k-fold cross validation).
2. for i: 1 to Q
* get a ROSE sample T*im from Tn \ TiK
* estimate the classifier on Tim
* make a prediction on PiK on TiK

3. compute the accuracy of the P1...PQ

## Logistic Regression


#### FROM OFFICE HOURS: If we see a distribution in the test data set of % positive. See whether tuning to get the same % positive on testing data.

Per our course's Module 3 instruction, logistic regression is typically used when there are 2 classes. I will be using the **haitiBinary** dataframe with two classes:
<br />
* **0**: Not a Blue Tarp, i.e. negative
* **1**: Is a Blue Tarp, i.e. positive

Logistic Regression with ROSE library

And the cv.glm() function from the boot library can carry out cross-validation. 

Dr. Woo's code for cv.glm comparison:
```{r}
#glm.fit<-glm(y~x2+x7+x8, data=data)

#cv.err<-cv.glm(data, glm.fit)

#cv.err$delta[1] ##the output for the LOOCV should match your own

#cv.glm(data, glm.fit, K=10)$delta[1] ##k fold CV with k=28
#cv.glm(data, glm.fit, K=28)$delta[1] ##k fold CV with k=28
```


```{r}
library("ROSE")
library(caret)
library(boot)
```

## 80/20 Train/Test Split
```{r haitiBinaryTrainTestSplit}
set.seed(1976)
sample_size = floor(0.8*nrow(haitiBinary))
# randomly split data in r
picked = sample(seq_len(nrow(haitiBinary)),size = sample_size)
train = haitiBinary[picked,]
test = haitiBinary[-picked,]
```


```{r}
# check imbalance
table(train$ClassBinary)
```


``` {r}
# train logistic regression on imbalanced data
log.reg.imb <- glm(ClassBinary ~ Blue+Green+Red, data=train, family=binomial())
```

``` {r}
# use the trained model to predict test data
pred.log.reg.imb <- predict(log.reg.imb, newdata=test,type="response")

confusion_matrix(log.reg.imb)
```

``` {r}
# generate new balanced data by ROSE
haitiBinary.rose <- ROSE(ClassBinary ~ Blue+Green+Red, data=train, seed=123)$data
```

``` {r}
# check (im)balance of new data
table(haitiBinary.rose$ClassBinary)
```

``` {r}
# train logistic regression on balanced data
log.reg.bal <- glm(ClassBinary ~ Blue+Green+Red, data=haitiBinary.rose, family=binomial)
```


``` {r}
# use the trained model to predict test data
pred.log.reg.bal <- predict(log.reg.bal, newdata=test, type="response")
```




``` {r}
# check accuracy of the two learners by measuring auc
roc.curve(test$ClassBinary, pred.log.reg.imb)
roc.curve(test$ClassBinary, pred.log.reg.bal, add.roc=TRUE, col=2)
```
### ROC Curve:
y axis ==  sensitivity
x-axis== false positive


``` {r}
# determine bootstrap distribution of the AUC of logit models
# trained on ROSE balanced samples
# B has been reduced from 100 to 10 for time saving solely
#boot.auc.bal <- ROSE.eval(ClassBinary~Blue+Green+Red, data=train, learner= glm, method.assess = "BOOT",control.learner=list(family=binomial),trace=TRUE, B=100)
#summary(boot.auc.bal)
```

## TEST THE ACCURACY, Works best with a threshold of 0.8 
**Need to** show some information on why 0.8 threshold seems the best

```{r}
accuracy.meas(test$ClassBinary, pred.log.reg.imb, threshold = 0.8)

```


```{r}
# compute accuracy measures (test set)
accuracy.meas(test$ClassBinary, pred.log.reg.bal, threshold = 0.8)
```
Precision == positive predictive value (True Positive Predictions / All Predicted Positives)

Recall == Sensitivity describes the proportion of Non-Blue Tarp that have been predicted correctly

```{r}
summary(log.reg.bal)
```



```{r}
exp(0.09865)
```

```{r}
confusion_matrix(log.reg.bal)
```

A one unit increase in Blue unit value, when Red and Green are held constant, is associated with a log odds Blue Tarp identification of 0.099 units, or an increase in odds of Blue Tarp of 10%.

We still have 2771 Blue Tarps that we not identified with this model.


```{r include=FALSE, echo=FALSE, render=FALSE}
# method: The resampling method
# number: number of folds for cross validation
ctrl <- trainControl(method = "repeatedcv", 
                     number = 10, 
                     repeats = 10, 
                     verboseIter = FALSE,
                     sampling = "rose")

set.seed(42)

# method: regression model to use
# for logistic I selected glmnet
# no pre-processing is needed
model_log_rose <- caret::train(ClassBinary ~ Blue+Red+Green,
                              data = haitiBinary,
                              method = "glmnet",
                              #preProcess = c("scale", "center"),
                              trControl = ctrl)

final_log_rose <- data.frame(actual = test_data$ClassBinary,
                         predict(model_log_rose, newdata = test_data, type = "prob"))
final_rose$predict <- ifelse(final_log_rose$1 > 0.5, "Blue Tarp", "Other")

'''
model_rf_rose <- caret::train(ClassBinary ~ Blue+Red+Green,
                              data = haitiBinary,
                              method = "rf",
                              #preProcess = c("scale", "center"),
                              trControl = ctrl)
'''

```



## LDA

## QDA

## KNN

### Tuning Parameter $k$

How were tuning parameter(s) selected? What value is used? Plots/Tables/etc.

## Penalized Logistic Regression (ElasticNet)

### Tuning Parameters

**NOTE: PART II same as above plus add Random Forest and SVM to Model Training.**

## Threshold Selection


# Results (Cross-Validation)

** CV Performance Table Here**


# Conclusions

### Conclusion \#1 

### Conclusion \#2

### Conclusion \#3


```{r, echo=FALSE}
# knitr::knit_exit()    # ignore everything after this
## Uncomment this line for Part I
## You can remove the entire code chunk for Part II
```


# Hold-out Data / EDA

Load data, explore data, etc. 


# Results (Hold-Out)

**Hold-Out Performance Table Here**


# Final Conclusions

### Conclusion \#1 

### Conclusion \#2

### Conclusion \#3

### Conclusion \#4 

### Conclusion \#5

### Conclusion \#6

