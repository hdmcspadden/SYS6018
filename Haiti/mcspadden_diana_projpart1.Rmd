---
title: "Disaster Relief Project: Part I"
author: "H. Diana McSpadden"
date: "`r format(Sys.Date(), '%b %d, %Y')`"
output:
  html_document:
    number_sections: true    
    toc: true
    toc_float: true
    theme: cosmo
    highlight: espresso    
 
---

<!--- Below are global settings for knitr. You can override any of them by adding the changes to individual chunks --->

```{r global_options, include=FALSE}
knitr::opts_chunk$set(error=TRUE,        # Keep compiling upon error
                      collapse=FALSE,    # collapse by default
                      echo=TRUE,         # echo code by default
                      comment = "#>",    # change comment character
                      fig.width = 5.5,     # set figure width
                      fig.align = "center",# set figure position
                      out.width = "49%", # set width of displayed images
                      warning=TRUE,      # show R warnings
                      message=TRUE)      # show R messages
```

<!--- Change font sizes (or other css modifications) --->
<style>
h1.title {
  font-size: 2.2em; /* Title font size */
}
h1 {
  font-size: 2em;   /* Header 1 font size */
}
h2 {
  font-size: 1.5em;
}
h3 { 
  font-size: 1.2em;
}
pre {
  font-size: 0.8em;  /* Code and R output font size */
}
</style>



**SYS 6018 | Spring 2021 | University of Virginia **

*******************************************
# SYS 6018: Disaster Relief Project
## R-G-B for Dis-Cov-Er-Y

### H. Diana McSpadden

# Introduction 

When natural disasters or other emergencies result in compromised communications and logistical challenges, it can be impossible within a reasonable time to locate survivors using human-only methods. This disaster relief toy problem seeks to discover if an algorithm can effectively identify images corresponding to people who require relief.

To investigate whether this is possible a data set was provided of RGB color values corresponding to pixels. The pixels are from high-resolution images taken by an aircraft above Haiti. Blue tarps had been distributed to survivors, but rescue workers did not have information about where survivors were located after receiving the blue tarps. 

Blue tarps have a distinct color when compared to other identifiable elements in Haiti. The training data set includes 63,241 RGB values for pixels in the toy problem's images. The training data set includes a label for each pixel. The labels include:
<ul>
<li>Blue Tarp</li>
<li>Rooftop</li>
<li>Soil</li>
<li>Various Non-Tarp</li>
<li>Vegetation</li>
</ul>

Below the training data is explored, and various modeling methods applied to determine if, and which method can be effectively used to identify blue tarps by RGB values.

# Training Data / EDA

Load data, explore data, etc. 
```{r echo=FALSE}
#install.packages("pROC")
#install.packages("randomForest")
#install.packages("reticulate")
#install("regclass")
```


```{r load-packages, warning=FALSE, message=FALSE}
# Load Required Packages
library(tidyverse)
library(pROC)
library(randomForest)
library("GGally")
library(gridExtra)
library(plotly)
#library(reticulate)
library(regclass)
#library(ROSE)
library(MLeval)
library(ggplot2)
library(purrr)
library(broom)
```

```{r load-data, message=FALSE, warning=FALSE}
url = 'HaitiPixels.csv'
#url = 'https://collab.its.virginia.edu/access/lessonbuilder/item/1707832/group/17f014a1-d43d-4c78-a5c6-698a9643404f/Module3/HaitiPixels.csv' #this url is beng 
haiti <- read_csv(url)
print(dim(haiti))

```
```{r data-head1}
head(haiti)
```

The dataframe contains 4 columns, and 63,241 rows. The Class column contains the correct label for the observation. Red, Green and Blue parameters are **NEED TO INCLUDE CORRECT DEFINITION**

## Class Factor
To prepare the data for exploratory data analysis I make Class a factor.

```{r factorClass}
haiti %>% 
  mutate(Class = factor(Class)) 
```

Examine the numbers and percentages in each of the **5** classes:
```{r summarizeByCategory}
haiti %>%
  group_by(Class) %>%
  summarize(N = n()) %>%
  mutate(Perc = round(N / sum(N), 2) * 100)
```

#### Observations:
The records are not evenly distributed between the categories. Of the Classes Blue Tarp, our "positive" category if we are thinking a binary positive/negative identification, is only 3% of our sample. Soil and Vegetation make up the majority of our sample at 74%.

## Binary Class Factor vs. 5 Class Factor
It will be interesting to see performance predicting each of these categories, or a binary is or is not Blue Tarp.

### Create Binary DataFrame

Create a DataFrame that is only Blue Tarp, or not Blue Tarp:
<ul>
<li> 0 == Not a Blue Tarp</li>
<li> 1 == Is a Blue Tarp</li>
</ul>

After reviewing box plots for the 2-class data set, I also created two new calculated variables:
<br />
1. GBSqr = (Green + Blue)^2 * .001
<br />
2. RBSqr = (Red + Blue)^2 * .001

I created these to continue using the Red and Green values, but I wanted to increase the difference in median value difference between the positive and negative classes. There is significant interplay in color values between Red, Green, and Blue in identifying the correct shade or blue, and I wanted to continue using Red and Green values but increase the linear separability between the classes. The 0.01 multiplier is to return the number scale to a range similar to standard RGB values.
```{r createBinaryDataFrame}
haitiBinary =  haiti %>%
  mutate(ClassBinary = if_else(Class == 'Blue Tarp', '1', '0'), ClassBinary = factor(ClassBinary))

haitiBinarySqrs = haiti %>%
  mutate(GBSqr = I(((Green + Blue)^2) * .001), RBSqr = I(((Red + Blue)^2) * .001), ClassBinary = if_else(Class == 'Blue Tarp', '1', '0'), ClassBinary = factor(ClassBinary))

```


Examine the numbers and percentages in each of the **2** classes:
```{r summarizeBinaryByCategory}

haitiBinary %>%
  group_by(ClassBinary) %>%
  summarize(N = n()) %>%
  mutate(Perc = round(N / sum(N), 2) * 100)

```

### How are red, blue and green values distributed between the **5** classes?

```{r colorValuesFiveCats, fig.height=5, fig.width=8}

redplot <- ggplot(haiti, aes(x=Class, y=Red)) + 
  geom_boxplot(col='red')

greenplot <- ggplot(haiti, aes(x=Class, y=Green)) + 
  geom_boxplot(col='darkgreen')

blueplot <- ggplot(haiti, aes(x=Class, y=Blue)) + 
  geom_boxplot(col='darkblue')

grid.arrange(redplot, greenplot, blueplot)
```


### How are red, blue and green values distributed between the **2** classes?

```{r colorValuesBinaryCats, fig.height=5, fig.width=5}

redplotB <- ggplot(haitiBinary, aes(x=ClassBinary, y=Red)) + 
  geom_boxplot(col='red')

greenplotB <- ggplot(haitiBinary, aes(x=ClassBinary, y=Green)) + 
  geom_boxplot(col='darkgreen')

blueplotB <- ggplot(haitiBinary, aes(x=ClassBinary, y=Blue)) + 
  geom_boxplot(col='darkblue')

grid.arrange(redplotB, greenplotB, blueplotB)
```
### How are red, blue and green values distributed between the **2** classes with the square values for Red + Blue and Green Blue?

```{r colorValuesBinarySqrsCats, fig.height=5, fig.width=5}

redplotB <- ggplot(haitiBinarySqrs, aes(x=ClassBinary, y=RBSqr)) + 
  geom_boxplot(col='red')

greenplotB <- ggplot(haitiBinarySqrs, aes(x=ClassBinary, y=GBSqr)) + 
  geom_boxplot(col='darkgreen')

blueplotB <- ggplot(haitiBinarySqrs, aes(x=ClassBinary, y=Blue)) + 
  geom_boxplot(col='darkblue')

grid.arrange(redplotB, greenplotB, blueplotB)
```



#### Box Plot Observations

**For the 5-class box plots:**

"Blue Tarp" as the "positive" result, and other results as the "negative" result.

Regarding the box plot of the five categories, of interest is that "Soil" and "Vegetation" are relatively unique in their RGB distributions. "Rooftop" and "Various Non-Tarp" are more similar in their RBG distributions

**For the 2-class box plots:**

If the classes are collapsed to binary values of "Blue Tarp (1)" and "Not Blue Tarp (0)" there is little overlap in the blue values for the two classes, and the ranges of red and green are much smaller for blue tarp than non-blue-tarp.
<br /><br />
Generally, the values of red have a larger range for negative results than for positive results, and the positive results have a similar median to the negative results.
<br /></br>
Green values have a larger range for negative results than for positive results, and the positive results have a higher median than the negative results. 
<br /><br />
There is ***almost*** **no overlap in the blue data** with non-blue tarps, and blue tarps.

**For the 2-class box plots with the additive square values:**

If the classes are collapsed to binary values of "Blue Tarp (1)" and "Not Blue Tarp (0)" there is little overlap in the blue values for the two classes, and the RBSqr and GBSqr values have much less overlap than without the additive square variables.
<br /><br />
The values of RBSqr have a larger range for negative results than for negative results, and median is significantly greater in the positive results.
<br /></br>
GBSqr values have a larger range for negative results than for positive results. The positive results have a significantly higher median than the negative results. 
<br /><br />
There is ***almost*** **no overlap in the blue data** with non-blue tarps, and blue tarps.

### View the correlation between Red, Green and Blue

These correlations make sense as the pixels were of highly saturated colors, that are not pure Blue, Red or Green. There are few pixels in the data set with low values for R,G,B.

```{r }
#ggpairs(haiti, lower = list(continuous = "points", combo = "dot_no_facet"), progress = F)
ggpairs(haiti, progress = F)
```

```{r }
#ggpairs(haiti, lower = list(continuous = "points", combo = "dot_no_facet"), progress = F)
ggpairs(haitiBinary[-1], progress = F)
```


```{r}
ggpairs(haitiBinarySqrs[-1], progress = F)
```
The RBSqr and GBSqr have significantly less variance in their values, and better differentiation between the 2 classes than the Red and Green variables. I will be using these transformed variables in my models.

### 3-D Scatterplot

To view the relationship between the Red, Green, and Blue values between the five classes, and the binary classes, an interactive 3-D scatter plot is illustrative.


#### Five-Class 3-D Scatterplot
```{r 3D5Class}

fiveCat3D = plot_ly(x=haiti$Red, y=haiti$Blue, z=haiti$Green, type="scatter3d", mode="markers", color=haiti$Class, colors = c('blue2','azure4','chocolate4','coral2','chartreuse4'),
marker = list(symbol = 'circle', sizemode = 'diameter', opacity =0.35))

fiveCat3D = fiveCat3D %>%
  layout(title="5 Category RBG Plot", scene = list(xaxis = list(title = "Red", color="red"), yaxis = list(title = "Blue", color="blue"), zaxis = list(title = "Green", color="green")))

fiveCat3D

```
**5-Class 3-D Scatter Plot Observations**
<br />
One can see that there are discernible groupings of pixel categories by RGB values. Unsurprisingly, the blue tarps are higher blue values, but they do have a range of red and green values.
<br /><br />

The 3D scatter plot is particularly useful because, by zooming in, one can see that while the 'Blue Tarp' values are generally distinct, there is a space in the 3D plot with mingling of "blue tarp" pixels and other pixel categories. That area of the data will provide a challenge for our model.

##### Two-Class 3-D Scatterplot

```{r 3DBinary}
binary3D = plot_ly(x=haitiBinarySqrs$RBSqr, y=haitiBinarySqrs$Blue, z=haitiBinarySqrs$GBSqr, type="scatter3d", mode="markers", color=haitiBinary$ClassBinary, colors = c('red','blue2'),
marker = list(symbol = 'circle', sizemode = 'diameter', opacity =0.35))

binary3D = binary3D %>%
  layout(title="Binary RBG Plot", scene = list(xaxis = list(title = "RBSqr", color="red"), yaxis = list(title = "Blue", color="blue"), zaxis = list(title = "GBSqr", color="green")))

binary3D
```
**2-Class 3-D Scatter Plot Observations With Blue, GBSqr, and RBSqr**
<br />
Similar to the five category 3D scatter plot, the binary scatter plot shows distinct groupings for blue tarp and non-blue-tarp. There is a clear linear boundary between the blue tarp and non-blue tarp observations. 


### Parameter Selection:
Based on EDA, I am hopeful that my models will perform well using the following predictors:
<ol>
<li>Red</li>
<li>Green</li>
<li>Blue</li>
<li>GBSqr: ((Green + Blue)^2) * .001</li>
<li>RBSqr: ((Red + Blue)^2) * .001</li>
</ol>


# Model Training

## Set-up 

Normalization does not need to be considered because the ranges of Red, Green and Blue are the same.
<br /><br/>
I am using the 2-Class data set for the following reasons:
<ol>
<li>The distinctions in the 2-Class data set, as seen in the 3-D scatterplot, are clear.</li>
<li>The stated problem is to classify 'Blue Tarp' from the other classes. Classifying the other classes is not of interest.</li>
<li>I am using 10-fold cross-validation to evaluate the models.</li>
</ol>

<br /><br />


### Training and Test Data: 

I will hold out 20% of the data set for testing/validation.

```{r warning=FALSE, message=FALSE}
library(caret)
library(boot)
```


## Cross-Validation Performance 

For logistic regression, LDA, QDA, and **???KNN** Cross-Validation threshold performance used ROC for tuning.

The following performance measures are collected for both the 10-fold cross-validation and the hold-out/testing/validation data:
<ul>
<li>AUROC</li>
<li>True Positive Rate</li>
<li>False Positive Rate</li>
<li>Precision</li>
</ul>

<br /><br />
For the Models:
* **No**: Not a Blue Tarp is **Negative**
* **Yes**: Is a Blue Tarp is **Positive**

## Logistic Regression

Per our course's Module 3 instruction, logistic regression is typically used when there are 2 classes. I will be using the **haitiBinary** dataframe with two classes:
<br /><br />

Reset the level names to enable the caret functions for the ROC curve.

```{r}
levels(haitiBinarySqrs$ClassBinary)

levels(haitiBinarySqrs$ClassBinary)=c("No","Yes")

levels(haitiBinarySqrs$ClassBinary)

fct_count(haitiBinarySqrs$ClassBinary)

train = haitiBinarySqrs
```

Here I perform my own 10-fold cross validation for several values of p

```{r logRegressionTuning, warning=FALSE}
library(yardstick)

set.seed(1976)
#Randomly shuffle the data
haitiBinarySqrsShuffled = haitiBinarySqrs[sample(nrow(haitiBinarySqrs)),]

#Create 10 equally size folds
folds <- cut(seq(1,nrow(haitiBinarySqrsShuffled)),breaks=10,labels=FALSE)


# create storage variables for the p value, ROC, Specificity, and Sensitivity
k_start = 1
k_end = 10
out_nvar = k_end - k_start + 1

p_i = rep(NA, out_nvar)
sensitivity_i = rep(NA, out_nvar)
specificity_i = rep(NA, out_nvar)
prec_i = rep(NA, out_nvar)

counter = 1

for (j in c(.5,.55,.6,.65,.7,.75,.8,.85,.9,.95))
{ 

  p_i[counter] = j
  
  accumulator_sens = 0
  accumulator_spec = 0
  accumulator_prec = 0
  
  #Perform 10 fold cross validation
  for(i in 1:10) {
    
      #Segement your data by fold using the which() function 
      testIndexes <- which(folds==i,arr.ind=TRUE)
      testData <- haitiBinarySqrsShuffled[testIndexes, ]
      trainData <- haitiBinarySqrsShuffled[-testIndexes, ]
      
      glm.fits <- glm(ClassBinary ~ Blue+Green+Red+GBSqr+RBSqr, binomial, data = trainData)
      
      glm.logOdds_predictions = glm.fits %>%  augment(newdata=testData) %>% 
      dplyr::select(ClassBinary, .fitted)  %>% 
      mutate(ClassBinary=factor(ClassBinary))%>%
      mutate(.prediction=ifelse(1 - 1/(1 + exp(.fitted)) < j, "No", "Yes")) %>%
      mutate(.prediction=fct_relevel(as_factor(.prediction), c("No", "Yes"))) 
      

      accumulator_sens = accumulator_sens + yardstick::sens(glm.logOdds_predictions, ClassBinary, .prediction)[[3]]
      accumulator_spec = accumulator_spec + yardstick::spec(glm.logOdds_predictions, ClassBinary, .prediction)[[3]]
      accumulator_prec = accumulator_prec + yardstick::precision(glm.logOdds_predictions, ClassBinary, .prediction)[[3]]
      
      #print(glm.logOdds_predictions %>% conf_mat(ClassBinary, .prediction))
  }
  
  sensitivity_i[counter] = accumulator_sens / 10
  specificity_i[counter] = accumulator_spec / 10
  prec_i[counter] = accumulator_prec /10
  
  counter = counter + 1
}

outcome = data.frame(p_i,sensitivity_i, specificity_i, prec_i)

outcome

```

For Logistic Regression, p-values of 0.5, 0.95 are interesting.
<ul>
<li>p = 0.5 provides the greatest Specificity and Precision</li>
<li>p = 0.95 provides the greatest Sensitivity</li>
</ul>

All the values are similar; however, for Logistic Regression, I am selecting the value with the greatest Precision, **p == 0.5.**

Fortunately, p = 0.5 is the default for Logistic Regression cross-validation.

### Logistic Regression Performance
10-fold cross-validation training resulted in a **best-threshold of XX** for the model when ROC was used as the performance metric.

```{r logisticTrainingPerf, warning=FALSE}
trctrl.log.selected <- trainControl(method = "repeatedcv", summaryFunction=twoClassSummary, classProbs=T, savePredictions = T)

log.cv.model.selected = train(ClassBinary ~ Blue+Green+Red+GBSqr+RBSqr, data = haitiBinarySqrs, method = "glm", trControl=trctrl.log.selected, tuneLength = 10)

caret::confusionMatrix(log.cv.model.selected)
```
For LDA, my calculations for TPR, FPR, and Precision treat 'Yes', i.e. "yes, it is a blue tarp", as the positive class.
<ul>
<li>TPR: 2.9 / 3.2 = 0.906</li>
<li>FPR: .2 / 96.8 = 0.002</li>
<li>Precision: 2.9 / 3.1 = 0.935</li>
</ul>


### Logistic Regression Cross-Validation ROC Curve:

```{r LogROCCurve}
result = evalm(log.cv.model.selected)
result$roc
```

**The Logistic Regression ROC-AUC for the 10-fold cross-validated training data is: 1.0.**

<!--## Logistic Regression: Testing -->

```{r include=FALSE, echo=FALSE, eval=FALSE}

#test.YesNo = test

#levels(test.YesNo$ClassBinary)

#levels(test.YesNo$ClassBinary)=c("No","Yes")

#levels(test.YesNo$ClassBinary)

#fct_count(test.YesNo$ClassBinary)
```


```{r logisticTestResults, include=FALSE, echo=FALSE, eval=FALSE}
#log.test = predict(log.cv.model, newdata = test.YesNo, type="prob") %>% 
#  mutate('ClassBinary'= names(.)[apply(., 1, which.max)])
```  
  
```{r include=FALSE, echo=FALSE, eval=FALSE}
#log.test = log.test %>% 
#  mutate(ClassBinary = factor(ClassBinary)) 
```

```{r include=FALSE, echo=FALSE, eval=FALSE}  
#caret::confusionMatrix(log.test$ClassBinary, test.YesNo$ClassBinary)
```

```{r logROCTest, include=FALSE, echo=FALSE, eval=FALSE}
#plot(roc.curve(test.YesNo$ClassBinary, log.test$ClassBinary)) # Base R version

#test.result = evalm(log.test)
```

## LDA

Train the LDA model using 10-fold cross validation. Tuning is performed using ROC.

10-fold cross validation was used for p-values from 0.05 to 1 at 0.05 intervals.

```{r ldaCrossValidation}
set.seed(1976)

# create storage variables for the p value, ROC, Specificity, and Sensitivity
k_start = 1
k_end = 20
out_nvar = k_end - k_start + 1

p_i = rep(NA, out_nvar)
sensitivity_i = rep(NA, out_nvar)
specificity_i = rep(NA, out_nvar)
ROC_i = rep(NA, out_nvar)

for (i in 1:20) 
{ 
  p_i[i] = i/20
  
  trctrl <- trainControl(method = "repeatedcv", summaryFunction=twoClassSummary, classProbs=T, savePredictions = T, p = i/20)
  
  lda.cv.model = train(ClassBinary ~ Blue+Green+Red+GBSqr+RBSqr, data = train, method = "lda", trControl=trctrl, tuneLength = 10)
  
  sensitivity_i[i] = lda.cv.model$results[['Sens']]
  specificity_i[i] = lda.cv.model$results[['Spec']]
  ROC_i[i] = lda.cv.model$results[['ROC']]
}

outcome = data.frame(p_i,sensitivity_i, specificity_i, ROC_i)

outcome

```
For LDA, p-values of 0.55, 0.8, and 1.0 are interesting.
<ul>
<li>p = 0.55 provides the greatest Sensitivity, or True Positive Rate of 0.992813</li>
<li>p = 0.8 demonstrates the greatest ROC of 0.9945253</li>
<li>p = 1.0, unsurprisingly, provides the greatest True Negative Rate. Unsurprisingly because it required the highest posterior probability for an observation to be classified as a blue tarp.</li>
</ul>

All the values are similar, and any of these three could be a valid choice based on considerations of the risk of missing a true blue tarp, which could result in the loss of human life by not identifying the location of survivors, or the risk of wasted time of aid workers by sending them to an incorrectly predicted location of a blue tarp.

I am selecting the value with the greatest ROC, **p == 0.80.**

```{r ldaTrainConfusionMatrix}
trctrl.lda.selected <- trainControl(method = "repeatedcv", summaryFunction=twoClassSummary, classProbs=T, savePredictions = T, p = 0.8)
  
#trctrl.lda.selected <- trainControl(method = "repeatedcv", summaryFunction=twoClassSummary, classProbs=T, savePredictions = T, p = 1.0)
lda.cv.model.selected = train(ClassBinary ~ Blue+Green+Red+GBSqr+RBSqr, data = train, method = "lda", trControl=trctrl.lda.selected, tuneLength = 10)

caret::confusionMatrix(lda.cv.model.selected)
```
For LDA, my calculations for TPR, FPR, and Precision treat 'Yes', i.e. "yes, it is a blue tarp", as the positive class.
<ul>
<li>TPR: 2.7 / 3.2 = 0.844</li>
<li>FPR: .1 / 96.8 = 0.001</li>
<li>Precision: 2.7 / 2.8 = 0.964</li>
</ul>


### LDA Training ROC Curve:
```{r ldaTrainingROC}
result.lda = evalm(lda.cv.model.selected)
result.lda$roq
```
**The LDA ROC-AUC for the 10-fold cross-validated training data with p=0.8 is: 0.99.**


```{r ldaTestResults, include=FALSE, echo=FALSE, eval=FALSE}
#lda.test = predict(lda.cv.model, newdata = test.YesNo, type="prob") %>% 
#  mutate('ClassBinary'= names(.)[apply(., 1, which.max)])
```  
  
```{r include=FALSE, echo=FALSE, eval=FALSE}
#lda.test = lda.test %>% 
#  mutate(ClassBinary = factor(ClassBinary)) 
```

```{r include=FALSE, echo=FALSE, eval=FALSE}  
#caret::confusionMatrix(lda.test$ClassBinary, test.YesNo$ClassBinary)
```

```{r ldaROCTest, include=FALSE, echo=FALSE, eval=FALSE}
#test.lda.result = evalm(lda.test)
```

## QDA

Train the QDA model using 10-fold cross validation. Tuning performed using ROC.
```{r qdaCrossValidation}
set.seed(1976)

# create storage variables for the p value, ROC, Specificity, and Sensitivity
k_start = 1
k_end = 20
out_nvar = k_end - k_start + 1

p_i = rep(NA, out_nvar)
sensitivity_i = rep(NA, out_nvar)
specificity_i = rep(NA, out_nvar)
ROC_i = rep(NA, out_nvar)

for (i in 1:20) 
{ 
  p_i[i] = i/20
  
  trctrl <- trainControl(method = "repeatedcv", summaryFunction=twoClassSummary, classProbs=T, savePredictions = T, p = i/20)
  
  qda.cv.model = train(ClassBinary ~ Blue+Green+Red+GBSqr+RBSqr, data = train, method = "qda", trControl=trctrl, tuneLength = 10)
  
  sensitivity_i[i] = qda.cv.model$results[['Sens']]
  specificity_i[i] = qda.cv.model$results[['Spec']]
  ROC_i[i] = qda.cv.model$results[['ROC']]
}

outcome = data.frame(p_i,sensitivity_i, specificity_i, ROC_i)

outcome
```
For QDA, the following p-values are interesting
<ul>
<li>p = 0.8, 0.9, and 1.0 all provide the highest sensitivity of 0.9978</li>
<li>p = 0.65, 0.70, 0.85. and 0.95 all provide similar, high ROCs' ~0.9974</li>
<li>p = 0.85 provides the greatest specificity.</li>
</ul>

I will select p = 0.85 for the QDA model as it has near the greatest ROC and has the greatest specificity.

```{r qdaTrainConfusionMatrix}
trctrl.qda.selected <- trainControl(method = "repeatedcv", summaryFunction=twoClassSummary, classProbs=T, savePredictions = T, p = 0.85)
  
qda.cv.model.selected = train(ClassBinary ~ Blue+Green+Red+GBSqr+RBSqr, data = train, method = "qda", trControl=trctrl.lda.selected, tuneLength = 10)

caret::confusionMatrix(qda.cv.model.selected)
```
For QDA, my calculations for TPR, FPR, and Precision treat 'Yes', i.e. "yes, it is a blue tarp", as the positive class.
<ul>
<li>TPR: 2.9 / 3.2 = 0.906</li>
<li>FPR: .2 / 96.8 = 0.002</li>
<li>Precision: 2.9 / 3.0 = 0.97</li>
</ul>


### QDA Training ROC Curve:
```{r qdaTrainingROC}
result.qda = evalm(qda.cv.model.selected)
result.qda$roc
```

```{r qdaTestResults, include=FALSE, echo=FALSE, eval=FALSE}
#qda.test = predict(qda.cv.model, newdata = test.YesNo, type="prob") %>% 
#  mutate('ClassBinary'= names(.)[apply(., 1, which.max)])
```  
  
```{r qdaFactor, include=FALSE, echo=FALSE, eval=FALSE}
#qda.test = qda.test %>% 
#  mutate(ClassBinary = factor(ClassBinary)) 
```

```{r qdaTestCM, include=FALSE, echo=FALSE, eval=FALSE}  
#caret::confusionMatrix(qda.test$ClassBinary, test.YesNo$ClassBinary)
```


```{r qdaROCTest, include=FALSE, echo=FALSE, eval=FALSE}
#test.qda.result = evalm(qda.test)
```

## KNN

### Tuning Parameter $k$

```{r knnTry1-21}
set.seed(1976)

knn.cv.model = train(ClassBinary ~ Blue+Green+Red+GBSqr+RBSqr, data = train, method = "knn", trControl=trctrl, tuneGrid   = expand.grid(k = 1:21))
knn.cv.model
```

```{r knnTry17-35}
set.seed(1976)

knn.cv.model = train(ClassBinary ~ Blue+Green+Red+GBSqr+RBSqr, data = train, method = "knn", trControl=trctrl, tuneGrid   = expand.grid(k = 17:35))
knn.cv.model
```

```{r knnTry27-51}
set.seed(1976)

knn.cv.model = train(ClassBinary ~ Blue+Green+Red+GBSqr+RBSqr, data = train, method = "knn", trControl=trctrl, tuneGrid   = expand.grid(k = 27:51))
knn.cv.model

```


```{r knnTrainConfusionMatrix}
caret::confusionMatrix(knn.cv.model)
```
### KNN k=17 Training ROC Curve:
```{r knnTrainingROC}
result.knn = evalm(knn.cv.model)
result.knn$roc
```


### How were tuning parameter(s) selected? What value is used? Plots/Tables/etc.

I ran 10-fold cross-validation for several ranges of k:
<ul>
<li>1 - 21: Returned best k == ***17***</li>
<li>17 - 35: Returned best k == ***27***</li>
<li>27 - 51: Returned best k == ***27***</li>
</ul>

<br /><br />
From 1 - 51, the best k is **27**. The tables of ROC, Sensitivity and Specificity were reviewed for each cross-validation training. From these tables one can see that the improvements within the range are in the hundredths of a percentage point of ROC, so k's in the range of 10 - 51, are all reasonable selections for the cross-validated training data.


## Penalized Logistic Regression (ElasticNet)

```{r}
set.seed(1976)
# number: number of folds for cross validation
trctrl <- trainControl(method = "repeatedcv", summaryFunction=twoClassSummary, classProbs=T, savePredictions = T, number = 10, repeats = 2)

log.cv.model = train(ClassBinary ~ Blue+Green+Red+GBSqr+RBSqr, data = train, method = "glmnet", trControl=trctrl, tuneLength = 10)
log.cv.model

```

### Tuning Parameters



**NOTE: PART II same as above plus add Random Forest and SVM to Model Training.**

## Threshold Selection

# Results (Cross-Validation)

<table width="800">
<th>
  <td width="100">Model</td>
  <td width="100">Tuning</td>
  <td width="100">AUROC</td>
  <td width="100">Threshold</td>
  <td width="100">Accuracy</td>
  <td width="100">TPR</td>
  <td width="100">FPR</td>
  <td width="100">Precision</td>
</th>

<tr>
  <td>Log Reg</td>
  <td><!-- Log Reg Tuning --> N/A</td>
  <td><!-- Log Reg AUROC -->1.0</td>
  <td><!-- Log Reg Threshold -->0.5</td>
  <td><!-- Log Reg Accuracy -->0.9953</td>
  <td><!-- Log Reg TPR -->0.906</td>
  <td><!-- Log Reg FPR -->0.002</td>
  <td><!-- Log Reg Precision -->0.935</td>
</tr>

<tr>
  <td>LDA</td>
  <td><!-- LDA Tuning --> N/A</td>
  <td><!-- LDA AUROC -->0.99</td>
  <td><!-- LDA Threshold -->0.8</td>
  <td><!-- LDA Accuracy -->0.994</td>
  <td><!-- LDA TPR -->0.844</td>
  <td><!-- LDA FPR -->0.001</td>
  <td><!-- LDA Precision -->0.964</td>
</tr>

<tr>
  <td>QDA</td>
  <td><!-- QDA Tuning --> N/A</td>
  <td><!-- QDA AUROC -->1.0</td>
  <td><!-- QDA Threshold -->0.85</td>
  <td><!-- QDA Accuracy -->0.995</td>
  <td><!-- QDA TPR -->0.906</td>
  <td><!-- QDA FPR -->0.002</td>
  <td><!-- QDA Precision -->0.97</td>
</tr>

<tr>
  <td>KNN</td>
  <td><!-- KNN Tuning --> k = 27</td>
  <td><!-- KNN AUROC -->1.0</td>
  <td><!-- KNN Threshold -->0.5</td>
  <td><!-- KNN Accuracy -->0.997</td>
  <td><!-- KNN TPR -->0.998</td>
  <td><!-- KNN FPR -->0.0313</td>
  <td><!-- KNN Precision -->0.999</td>
</tr>

<tr>
  <td>Penalized Log Reg</td>
  <td><!-- Penalized Log Reg Tuning --></td>
  <td><!-- Penalized Log Reg AUROC --></td>
  <td><!-- Penalized Log Reg Threshold --></td>
  <td><!-- Penalized Log Reg Accuracy --></td>
  <td><!-- Penalized Log Reg TPR --></td>
  <td><!-- Penalized Log Reg FPR --></td>
  <td><!-- Penalized Log Reg Precision --></td>
</tr>

<tr>
  <td>Random Forest</td>
  <td><!-- Random Forest Tuning --> </td>
  <td><!-- Random Forest AUROC --></td>
  <td><!-- Random Forest Threshold --></td>
  <td><!-- Random Forest Accuracy --></td>
  <td><!-- Random Forest TPR --></td>
  <td><!-- Random Forest FPR --></td>
  <td><!-- Random Forest Precision --></td>
</tr>

<tr>
  <td>SVM</td>
  <td><!-- SVM Tuning -->  </td>
  <td><!-- SVM AUROC --></td>
  <td><!-- SVM Threshold --></td>
  <td><!-- SVM Accuracy --></td>
  <td><!-- SVM TPR --></td>
  <td><!-- SVM FPR --></td>
  <td><!-- SVM Precision --></td>
</tr>
</table>


# Conclusions

### Conclusion \#1 

**Training Data Linearly Separates**<br />

The training data linearly separates very well. Even the 5-class, un-transformed data set was visually separable in the 3-D R-G-B visualization of "Blue Tarps" vs. the other four classes. Collapsing the classes and transforming the variables to decrease variability of Red and Green further improved the linear separability. All the models used performed with 99% precision with ***training*** data and using the following function:
<br />
$$ ClassBinary = Blue+Green+Red+GBSqr+RBSqr$$
where $GBSqr=(Green + Blue)^2$ and $RBSqr=(Red + Blue)^2$
<br /><br />
I look forward to discovering if the hold-out/validation data set also separates as well.


### Conclusion \#2

**Distribution of Classes**<br/>

Only 3% of the observations in the training data are labeled "Blue Tarp". This is a very unbalanced training set. This is not unexpected because it would be extremely surprising is a high percentage of land area was covered by blue tarps. In fact, if that were the case, then I would expect it would not be challenging for aid workers to find survivors because survivors would be covering a high percentage of haiti. That said, this imbalance of the data can give a false sense of faith in a model. In this haiti toy problem, **a random guess of "not blue tarp" will be correct 97% of the time**, thus our model must perform better than a random guess. Additionally, our problem's aid workers must not have their limited time and resources expended on false blue tarps, so our False Positive Rate (FPR) must be kept low.


### Conclusion \#3

**Request For Additional Information for Real-World Implementation**<br />
I am interested in how to use limited resources to reach the greatest number of vulnerable people. Without the GIS information for each pixel I am unable to visualize and calculate locations where the most blue tarps will be found to able to help the most people. This assumes an identity relationship between number of tarps and number of people. My conclusion is that without the added GIS information for the observations I cannot provide information to help an efficient distribution of aid to those affected by the earthquake. Additionally, GIS information for pre-earthquake roads, and another model that could be used to whether corresponding pixels for pre-earthquake roads remain passable (could we use RBG values for intact paved roads, unpaved roads, and bridges) would be useful.

### Conclusion \#4

**Model Comments and Selection Based on Training Data**<br />

The models need to be validated using the hold-out test data, and I do not have information regarding the size of the training data vs. the size of real-life data, or the computing power of computers that would be running these models in a real-life scenario. Based on my computer's performance while running cross-validation with the various models, if performance time is a concern, the LDA and QDA models ran significantly faster than the other models. The QDA model performed better than the LDA model, which is surprising for two reasons:
<br />
1. the data visually appears linearly separable<br />
2. the high performance of the logistic regression model<br />
<br /><br/>
The QDA model performed with identical TPR (0.998), FPR (0.094), and Precision (0.997) to the logistic regression model, and ran faster. Thus, if computing time is of concern, I recommend the QDA model, but this may change after model testing with the hold-out data.

However, the KNN model with k == 27 produced the lowest FPR (0.03), and greatest Precision (0.999) which is particularly of interest because we do not want to spend precious time and people-power sending aid workers to locations were we mistakenly predict survivors. This KNN model performed very well, and if the size of n, computing power, and validation performance align, then this is my current recommended model.

## References
<ul>
<li>3D Scatter Plots in Python: <https://plotly.com/python/3d-scatter-plots/></li>
<li>Axes Labels in R: <https://plotly.com/r/figure-labels/></li>
<li>caret::trainControl: <https://www.rdocumentation.org/packages/caret/versions/6.0-86/topics/trainControl></li>
<li>Predictive Modeling with R and the caret Package: <http://www.edii.uclm.es/~useR-2013/Tutorials/kuhn/user_caret_2up.pdf></li>
<li>caret::train: <https://www.rdocumentation.org/packages/caret/versions/6.0-86/topics/train></li>
<li>MLeval::evalm: <https://www.rdocumentation.org/packages/MLeval/versions/0.3/topics/evalm></li>
</ul>
```{r, echo=FALSE}
knitr::knit_exit()    # ignore everything after this
## Uncomment this line for Part I
## You can remove the entire code chunk for Part II
```


# Hold-out Data / EDA

Load data, explore data, etc. 


# Results (Hold-Out)

# Results (Test/Validate Data)

<table width="800">
<th>
  <td width="100">Model</td>
  <td width="100">Tuning</td>
  <td width="100">AUROC</td>
  <td width="100">Threshold</td>
  <td width="100">Accuracy</td>
  <td width="100">TPR</td>
  <td width="100">FPR</td>
  <td width="100">Precision</td>
</th>

<tr>
  <td>Log Reg</td>
  <td><!-- Log Reg Tuning --> N/A</td>
  <td><!-- Log Reg AUROC -->TBD</td>
  <td><!-- Log Reg Threshold -->TBD</td>
  <td><!-- Log Reg Accuracy -->TBD</td>
  <td><!-- Log Reg TPR -->TBD</td>
  <td><!-- Log Reg FPR -->TBD</td>
  <td><!-- Log Reg Precision -->TBD</td>
</tr>

<tr>
  <td>LDA</td>
  <td><!-- LDA Tuning --> N/A</td>
  <td><!-- LDA AUROC -->TBD</td>
  <td><!-- LDA Threshold -->TBD</td>
  <td><!-- LDA Accuracy -->TBD</td>
  <td><!-- LDA TPR -->TBD</td>
  <td><!-- LDA FPR -->TBD</td>
  <td><!-- LDA Precision -->TBD</td>
</tr>

<tr>
  <td>QDA</td>
  <td><!-- QDA Tuning --> N/A</td>
  <td><!-- QDA AUROC -->TBD</td>
  <td><!-- QDA Threshold -->TBD</td>
  <td><!-- QDA Accuracy -->TBD</td>
  <td><!-- QDA TPR -->TBD</td>
  <td><!-- QDA FPR -->TBD</td>
  <td><!-- QDA Precision -->TBD</td>
</tr>

<tr>
  <td>KNN</td>
  <td><!-- KNN Tuning --> k = TBD</td>
  <td><!-- KNN AUROC -->TBD</td>
  <td><!-- KNN Threshold -->TBD</td>
  <td><!-- KNN Accuracy -->TBD</td>
  <td><!-- KNN TPR -->TBD</td>
  <td><!-- KNN FPR -->TBD</td>
  <td><!-- KNN Precision -->TBD</td>
</tr>

<tr>
  <td>Penalized Log Reg</td>
  <td><!-- Penalized Log Reg Tuning -->TBD</td>
  <td><!-- Penalized Log Reg AUROC -->TBD</td>
  <td><!-- Penalized Log Reg Threshold -->TBD</td>
  <td><!-- Penalized Log Reg Accuracy -->TBD</td>
  <td><!-- Penalized Log Reg TPR -->TBD</td>
  <td><!-- Penalized Log Reg FPR -->TBD</td>
  <td><!-- Penalized Log Reg Precision -->TBD</td>
</tr>

<tr>
  <td>Random Forest</td>
  <td><!-- Random Forest Tuning -->TBD</td>
  <td><!-- Random Forest AUROC -->TBD</td>
  <td><!-- Random Forest Threshold -->TBD</td>
  <td><!-- Random Forest Accuracy -->TBD</td>
  <td><!-- Random Forest TPR -->TBD</td>
  <td><!-- Random Forest FPR -->TBD</td>
  <td><!-- Random Forest Precision -->TBD</td>
</tr>

<tr>
  <td>SVM</td>
  <td><!-- SVM Tuning --> TBD</td>
  <td><!-- SVM AUROC -->TBD</td>
  <td><!-- SVM Threshold -->TBD</td>
  <td><!-- SVM Accuracy -->TBD</td>
  <td><!-- SVM TPR -->TBD</td>
  <td><!-- SVM FPR -->TBD</td>
  <td><!-- SVM Precision -->TBD</td>
</tr>
</table>


# Final Conclusions

### Conclusion \#1 

### Conclusion \#2

### Conclusion \#3

### Conclusion \#4 

### Conclusion \#5

### Conclusion \#6

