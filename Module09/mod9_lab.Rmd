---
title: "Mod9 Lab"
author: "Diana McSpadden"
date: "4/12/2021"
output: html_document
---
# Chapter 9 Lab: Support Vector Machines

# Support Vector Classifier

Uses soft margins and a linear separable hyperplane
```{r}
# x is a two columns matrix of random numbers == 2 feature variables
set.seed(1)
x=matrix(rnorm(20*2), ncol=2)

# y is the output variable
y=c(rep(-1,10), rep(1,10))

# shifting to right by one spot
x[y==1,]=x[y==1,] + 1

# concatenate with c
plot(x, col=(3-y))
dat=data.frame(x=x, y=as.factor(y))

```
```{r}
library(e1071)
```

fit our SVM with **linear kernel == suport vector classifier**
c is NOT the C value, this is the cost of observations violating the margins.
Higher cost == fewer violations.

X's are the support vectors
This does NOT show right or wrong.
``` {r}
svmfit=svm(y~., data=dat, kernel="linear", cost=10,scale=FALSE)
plot(svmfit, dat)
svmfit$index
summary(svmfit)
```

Run again with lower cost:
== more support vectors
``` {r}
svmfit=svm(y~., data=dat, kernel="linear", cost=0.1,scale=FALSE)
plot(svmfit, dat)
svmfit$index
```

Tuning with cross validation

WIth a range of costs.

``` {r}
set.seed(1)
tune.out=tune(svm,y~.,data=dat,kernel="linear",ranges=list(cost=c(0.001, 0.01, 0.1, 1,5,10,100)))
summary(tune.out)
bestmod=tune.out$best.model
summary(bestmod)
```
**Best Cost was 0.01**


### Build some new data for a TEST dataset
``` {r}
xtest=matrix(rnorm(20*2), ncol=2)
ytest=sample(c(-1,1), 20, rep=TRUE)
xtest[ytest==1,]=xtest[ytest==1,] + 1
testdat=data.frame(x=xtest, y=as.factor(ytest))

# apply predict to our new data
ypred=predict(bestmod,testdat)
table(predict=ypred, truth=testdat$y)
```
17 correct, 3 wrong.


**use a different value for cost**
``` {r}
svmfit=svm(y~., data=dat, kernel="linear", cost=.01,scale=FALSE)
ypred=predict(svmfit,testdat)
table(predict=ypred, truth=testdat$y)
x[y==1,]=x[y==1,]+0.5
plot(x, col=(y+5)/2, pch=19)
```
14 correct, 6 wrong with the **not best** cost.


#### Generate New Data
put new data in dataframe

Use a different cost 0.00001

Margin is small because so few support vectors (only 3)
``` {r}
dat=data.frame(x=x,y=as.factor(y))
svmfit=svm(y~., data=dat, kernel="linear", cost=1e5)
summary(svmfit)
plot(svmfit, dat)
```

More support vectors are probably more robust:
```{r}
svmfit=svm(y~., data=dat, kernel="linear", cost=1)
summary(svmfit)
plot(svmfit,dat)
```


# Support Vector Machine

Make a matrix of 220 observations.

Make some data.

Shift normal observations.

WE made 2D random normal variables.
We shifted some left
Shifted some right.
```{r}
set.seed(1)
x=matrix(rnorm(200*2), ncol=2)
x[1:100,]=x[1:100,]+2
x[101:150,]=x[101:150,]-2
y=c(rep(1,150),rep(2,50))
dat=data.frame(x=x,y=as.factor(y))
plot(x, col=y)
```

#### Create training data

User radial basis function for kernel.
Cost == 1
``` {r}
train=sample(200,100)
svmfit=svm(y~., data=dat[train,], kernel="radial",  gamma=1, cost=1)
plot(svmfit, dat[train,])
summary(svmfit)
```
THis gave us 31 support vectors.

Now, reduce the cost.

This is less robust - we know we should have near linear hyperplanes.
```{r}
svmfit=svm(y~., data=dat[train,], kernel="radial",gamma=1,cost=1e5)
plot(svmfit,dat[train,])
```


#### Tuning

tuning for cost and gamma (for radial basis function)
``` {r}
set.seed(1)
tune.out=tune(svm, y~., data=dat[train,], kernel="radial", ranges=list(cost=c(0.1,1,10,100,1000),gamma=c(0.5,1,2,3,4)))
summary(tune.out)
table(true=dat[-train,"y"], pred=predict(tune.out$best.model,newx=dat[-train,]))
```

```{r}
svmfit.opt = svm(y~., data = dat[train,], kernel = "radial", gamma=2, cost=1, decision.values=T)
fitted = attributes(predict(svmfit.opt,dat[train,], decision.values=TRUE))$decision.values
```


# ROC Curves

How to generate ROC curves:

NOTE: we switched the tpr, and fpr for the plot **BUT THE axis labels have not switched**

``` {r}
library(ROCR)
rocplot=function(pred, truth, ...){
   predob = prediction(pred, truth)
   perf = performance(predob, "fpr", "tpr")
   plot(perf,...)}
```



Fit the svm with previous data
``` {r}
svmfit.opt=svm(y~., data=dat[train,], kernel="radial",gamma=2, cost=1,decision.values=T)

fitted=attributes(predict(svmfit.opt,dat[train,],decision.values=TRUE))$decision.values

par(mfrow=c(1,2))

rocplot(fitted,dat[train,"y"],main="Training Data")
```
Need to switch axis

``` {r}
plot.new()
svmfit.flex=svm(y~., data=dat[train,], kernel="radial",gamma=50, cost=1, decision.values=T)

fitted=attributes(predict(svmfit.flex,dat[train,],decision.values=T))$decision.values
rocplot(fitted,dat[train,"y"],add=T,col="red")

fitted=attributes(predict(svmfit.opt,dat[-train,],decision.values=T))$decision.values
rocplot(fitted,dat[-train,"y"],main="Test Data")

fitted=attributes(predict(svmfit.flex,dat[-train,],decision.values=T))$decision.values
rocplot(fitted,dat[-train,"y"],add=T,col="red")
```

# SVM with Multiple Classes
If the response is a factor containing more than two levels, then the svm()
function will perform multi-class classification using the one-versus-one approach.
We explore that setting here by generating a third class of observations.

``` {r}
set.seed(1)
x=rbind(x, matrix(rnorm(50*2), ncol=2))
y=c(y, rep(0,50))

x[y==0,2]=x[y==0,2]+2
dat=data.frame(x=x, y=as.factor(y))

par(mfrow=c(1,1))
plot(x,col=(y+1))

svmfit=svm(y~., data=dat, kernel="radial", cost=10, gamma=1)
plot(svmfit, dat)
```
The e1071 library can also be used to perform support vector regression,
if the response vector that is passed in to svm() is numerical rather than a factor.

# Application to Gene Expression Data


``` {r}
library(ISLR)
names(Khan)
dim(Khan$xtrain)
dim(Khan$xtest)
length(Khan$ytrain)
length(Khan$ytest)
table(Khan$ytrain)
table(Khan$ytest)
```
This data set consists of expression measurements for 2,308 genes.
The training and test sets consist of 63 and 20 observations respectively.

We will use a support vector approach to predict cancer subtype using gene
expression measurements. In this data set, **there are a very large number of features relative to the number of observations. This suggests that we should use a linear kernel**, because the additional flexibility that will result
from using a polynomial or radial kernel is unnecessary.

``` {r}
dat=data.frame(x=Khan$xtrain, y=as.factor(Khan$ytrain))
out=svm(y~., data=dat, kernel="linear",cost=10)
summary(out)
table(out$fitted, dat$y)
```

``` {r}
dat.te=data.frame(x=Khan$xtest, y=as.factor(Khan$ytest))
pred.te=predict(out, newdata=dat.te)
table(pred.te, dat.te$y)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
